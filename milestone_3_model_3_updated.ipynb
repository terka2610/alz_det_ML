{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O4Yf-IoK0js"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "suy9G9kH0-lo",
    "outputId": "60c0122e-33dc-4125-980c-558fa0fa841c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyWavelets in /ext3/miniconda3/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /ext3/miniconda3/lib/python3.12/site-packages (from PyWavelets) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyWavelets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kSv3LGEP0-lq",
    "outputId": "5f59e009-71bf-4fe9-c579-d7ee19699d9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /ext3/miniconda3/lib/python3.12/site-packages (1.9.0)\n",
      "Requirement already satisfied: decorator in /ext3/miniconda3/lib/python3.12/site-packages (from mne) (5.1.1)\n",
      "Requirement already satisfied: jinja2 in /ext3/miniconda3/lib/python3.12/site-packages (from mne) (3.1.6)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /ext3/miniconda3/lib/python3.12/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.6 in /ext3/miniconda3/lib/python3.12/site-packages (from mne) (3.10.1)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /ext3/miniconda3/lib/python3.12/site-packages (from mne) (2.1.3)\n",
      "Requirement already satisfied: packaging in /ext3/miniconda3/lib/python3.12/site-packages (from mne) (24.2)\n",
      "Requirement already satisfied: pooch>=1.5 in /ext3/miniconda3/lib/python3.12/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.9 in /ext3/miniconda3/lib/python3.12/site-packages (from mne) (1.15.2)\n",
      "Requirement already satisfied: tqdm in /ext3/miniconda3/lib/python3.12/site-packages (from mne) (4.67.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /ext3/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.6->mne) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /ext3/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /ext3/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.6->mne) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /ext3/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /ext3/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.6->mne) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /ext3/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.6->mne) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /ext3/miniconda3/lib/python3.12/site-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /ext3/miniconda3/lib/python3.12/site-packages (from pooch>=1.5->mne) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /ext3/miniconda3/lib/python3.12/site-packages (from pooch>=1.5->mne) (2.32.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /ext3/miniconda3/lib/python3.12/site-packages (from jinja2->mne) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /ext3/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /ext3/miniconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /ext3/miniconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/lib/python3.12/site-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ALLwWHI00-lr",
    "outputId": "a30c9a47-57bd-482c-b951-dc508ae79458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /ext3/miniconda3/lib/python3.12/site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.24 in /ext3/miniconda3/lib/python3.12/site-packages (from scikit-image) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.11.4 in /ext3/miniconda3/lib/python3.12/site-packages (from scikit-image) (1.15.2)\n",
      "Requirement already satisfied: networkx>=3.0 in /ext3/miniconda3/lib/python3.12/site-packages (from scikit-image) (3.4.2)\n",
      "Requirement already satisfied: pillow>=10.1 in /ext3/miniconda3/lib/python3.12/site-packages (from scikit-image) (11.1.0)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /ext3/miniconda3/lib/python3.12/site-packages (from scikit-image) (2.37.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /ext3/miniconda3/lib/python3.12/site-packages (from scikit-image) (2025.3.13)\n",
      "Requirement already satisfied: packaging>=21 in /ext3/miniconda3/lib/python3.12/site-packages (from scikit-image) (24.2)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /ext3/miniconda3/lib/python3.12/site-packages (from scikit-image) (0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kqS6KVS9K0jo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import mne\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time # To measure time\n",
    "\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import detrend, butter, filtfilt\n",
    "import pywt\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_float, img_as_ubyte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgT81ewt0-lr"
   },
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hBAEgRdx0-ls"
   },
   "outputs": [],
   "source": [
    "def collect_eeg_data(folder_path):\n",
    "    data = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".set\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            raw = mne.io.read_raw_eeglab(file_path, preload=True)\n",
    "            data[filename] = raw.get_data()\n",
    "    return data\n",
    "\n",
    "def read_json_dicts(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data_dict = json.load(f)\n",
    "    return pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xH5v5FIEK0jt"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RD-HcUnY0-lt"
   },
   "outputs": [],
   "source": [
    "def preprocess_eegnet_minimal(eeg_data, fs, lowcut=1.0, highcut=40.0, order=5):\n",
    "    \"\"\"\n",
    "    Applies minimal preprocessing suitable for models like EEGNet:\n",
    "    Bandpass filtering and channel-wise standardization.\n",
    "\n",
    "    Args:\n",
    "        eeg_data (np.ndarray): Raw EEG data (n_channels, n_timesteps).\n",
    "        fs (float): Original sampling frequency.\n",
    "        lowcut (float): Lower cutoff frequency for bandpass filter (Hz).\n",
    "        highcut (float): Upper cutoff frequency for bandpass filter (Hz).\n",
    "        order (int): Order of the Butterworth filter.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed EEG data (n_channels, n_timesteps).\n",
    "    \"\"\"\n",
    "    n_channels, n_timesteps = eeg_data.shape\n",
    "    processed_data = np.zeros_like(eeg_data)\n",
    "\n",
    "    # 1. Bandpass Filter Design\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    # Ensure frequency bounds are valid\n",
    "    if low <= 0 or high >= 1:\n",
    "         print(f\"Warning: Filter frequencies ({lowcut}Hz, {highcut}Hz) are invalid for Nyquist freq {nyq}Hz. Adjusting or skipping filter.\")\n",
    "         # Option: Skip filtering or adjust bounds\n",
    "         b, a = None, None # Indicate filter skip\n",
    "    else:\n",
    "        try:\n",
    "            b, a = butter(order, [low, high], btype='band')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not design Butterworth filter (order={order}, freqs=[{low}, {high}]). Skipping filter. Error: {e}\")\n",
    "            b, a = None, None\n",
    "\n",
    "\n",
    "    # 2. Apply Filter and Standardize Channel by Channel\n",
    "    for i_ch in range(n_channels):\n",
    "        channel_data = eeg_data[i_ch, :]\n",
    "\n",
    "        # Apply filtering if filter design was successful\n",
    "        if b is not None and a is not None:\n",
    "             try:\n",
    "                 filtered_data = filtfilt(b, a, channel_data)\n",
    "             except Exception as e:\n",
    "                 print(f\"Warning: Filtering failed for channel {i_ch}. Using original data for this channel. Error: {e}\")\n",
    "                 filtered_data = channel_data # Use original if filtering fails\n",
    "        else:\n",
    "             filtered_data = channel_data # Use original if filter wasn't designed\n",
    "\n",
    "        # Standardize (z-score normalization)\n",
    "        mean = np.mean(filtered_data)\n",
    "        std = np.std(filtered_data)\n",
    "        if std > 1e-9: # Avoid division by zero\n",
    "            processed_data[i_ch, :] = (filtered_data - mean) / std\n",
    "        else:\n",
    "            processed_data[i_ch, :] = filtered_data - mean # Only center if std is zero\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aYbQDIy8K0ju"
   },
   "outputs": [],
   "source": [
    "def calculate_td_psd_features(epoch_data, fs, power_lambda=0.1, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Calculates the 7 TD-PSD features for a single EEG epoch (single channel).\n",
    "    Based on equations in Amini et al., 2021.\n",
    "\n",
    "    Args:\n",
    "        epoch_data (np.ndarray): 1D numpy array for a single channel epoch.\n",
    "        fs (float): Sampling frequency of the epoch data.\n",
    "        power_lambda (float): Lambda for power transform normalization.\n",
    "        epsilon (float): Small value to prevent log(0) or division by zero.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containing the 7 log-transformed TD-PSD features.\n",
    "                    Returns NaNs if calculation fails.\n",
    "    \"\"\"\n",
    "    n_samples = len(epoch_data)\n",
    "    if n_samples == 0:\n",
    "        return np.full(7, np.nan)\n",
    "\n",
    "    # Detrend the signal (optional but often good practice)\n",
    "    signal = detrend(epoch_data)\n",
    "\n",
    "    # 1. Calculate Power Spectrum and Moments\n",
    "    try:\n",
    "        # FFT\n",
    "        X = fft(signal)\n",
    "        # Power Spectrum (One-sided, ignoring DC for moments perhaps?)\n",
    "        # Frequencies for moments k: corresponds to frequency bins\n",
    "        freqs = np.fft.fftfreq(n_samples, 1/fs)\n",
    "        # Power spectrum P[k] = |X[k]|^2 / N\n",
    "        P = np.abs(X)**2 / n_samples\n",
    "\n",
    "        # Calculate moments m0, m2, m4\n",
    "        # m_n = sum(f^n * P(f)) df - approximated by sum(k^n * P[k])\n",
    "        # We use the magnitude of frequencies for k, ignore negative freqs?\n",
    "        # Let's use Hjorth parameters definition based on time-domain variance\n",
    "        # m0 = variance(signal) = total power (approx)\n",
    "        m0_bar = np.sum(signal**2) / n_samples # Variance = mean square if mean is zero\n",
    "        if m0_bar < epsilon: m0_bar = epsilon\n",
    "\n",
    "        # m2 = variance of first derivative (activity)\n",
    "        delta_x = np.diff(signal, n=1) * fs # Scale by fs? Hjorth doesn't explicitly scale by fs\n",
    "        m2_bar = np.sum(delta_x**2) / (n_samples -1) # Use n_samples-1?\n",
    "        if m2_bar < epsilon: m2_bar = epsilon\n",
    "\n",
    "\n",
    "        # m4 = variance of second derivative (mobility)\n",
    "        delta2_x = np.diff(signal, n=2) * (fs**2) # Scale by fs^2?\n",
    "        m4_bar = np.sum(delta2_x**2) / (n_samples -2)\n",
    "        if m4_bar < epsilon: m4_bar = epsilon\n",
    "\n",
    "\n",
    "        # Apply power transform (Box-Cox with lambda=0 is log, this is slightly different)\n",
    "        m0 = (m0_bar**power_lambda - 1) / power_lambda if power_lambda != 0 else np.log(m0_bar)\n",
    "        m2 = (m2_bar**power_lambda - 1) / power_lambda if power_lambda != 0 else np.log(m2_bar)\n",
    "        m4 = (m4_bar**power_lambda - 1) / power_lambda if power_lambda != 0 else np.log(m4_bar)\n",
    "\n",
    "        # Ensure moments are positive after transform for log\n",
    "        m0 = max(m0, epsilon)\n",
    "        m2 = max(m2, epsilon)\n",
    "        m4 = max(m4, epsilon)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating moments: {e}\")\n",
    "        return np.full(7, np.nan)\n",
    "\n",
    "    features = np.zeros(7)\n",
    "\n",
    "    # 2. Calculate Features f1, f2, f3\n",
    "    try:\n",
    "        features[0] = np.log(m0) # f1 = log(m0)\n",
    "        # Check for valid subtractions\n",
    "        if m0 <= m2: m0 = m2 + epsilon\n",
    "        if m0 <= m4: m0 = m4 + epsilon\n",
    "        features[1] = np.log(m0 - m2) # f2 = log(m0 - m2)\n",
    "        features[2] = np.log(m0 - m4) # f3 = log(m0 - m4)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error calculating f1, f2, f3: {e}\")\n",
    "         features[:3] = np.nan\n",
    "\n",
    "\n",
    "    # 3. Calculate Feature f4 (Sparseness)\n",
    "    try:\n",
    "        denominator_sqrt = np.sqrt(max(m0 - m2, epsilon)) * np.sqrt(max(m0 - m4, epsilon))\n",
    "        if denominator_sqrt < epsilon: denominator_sqrt = epsilon\n",
    "        features[3] = np.log(m0 / denominator_sqrt) # f4 = log(S) = log(m0 / sqrt((m0-m2)(m0-m4)))\n",
    "    except Exception as e:\n",
    "         print(f\"Error calculating f4 (Sparseness): {e}\")\n",
    "         features[3] = np.nan\n",
    "\n",
    "    # 4. Calculate Feature f5 (Irregularity Factor - IF)\n",
    "    # IF = (m4/m2) / (m2/m0) based on Hjorth parameters 'complexity'\n",
    "    # Paper formula: sqrt(m4/m2) / sqrt(m2/m0) => m0*m4 / m2^2\n",
    "    try:\n",
    "        if m2 < epsilon: m2 = epsilon\n",
    "        if_val = (m0 * m4) / (m2**2)\n",
    "        features[4] = np.log(max(if_val, epsilon)) # f5 = log(IF)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating f5 (IF): {e}\")\n",
    "        features[4] = np.nan\n",
    "\n",
    "    # 5. Calculate Feature f6 (Covariance - COV)\n",
    "    # COV = std_dev / mean\n",
    "    try:\n",
    "        mean_val = np.mean(signal)\n",
    "        std_dev_val = np.std(signal)\n",
    "        if abs(mean_val) < epsilon: mean_val = np.sign(mean_val) * epsilon if mean_val != 0 else epsilon\n",
    "        cov_val = std_dev_val / mean_val\n",
    "        features[5] = np.log(max(abs(cov_val), epsilon)) # Log of magnitude? Paper isn't explicit if COV can be negative. Let's take abs.\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating f6 (COV): {e}\")\n",
    "        features[5] = np.nan\n",
    "\n",
    "\n",
    "    # 6. Calculate Feature f7 (Teager Energy Operator - TEO)\n",
    "    try:\n",
    "        # TEO(x[j]) = x[j]^2 - x[j-1]x[j+1]\n",
    "        # Need to handle boundaries (pad or slice)\n",
    "        teo_vals = signal[1:-1]**2 - signal[:-2] * signal[2:]\n",
    "        sum_teo = np.sum(teo_vals)\n",
    "        features[6] = np.log(max(abs(sum_teo), epsilon)) # Log of magnitude? Sum can be negative. Paper isn't explicit. Taking abs.\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating f7 (TEO): {e}\")\n",
    "        features[6] = np.nan\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R6Piy2l30-lu"
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_eegnet_minimal(eeg_data, fs, lowcut=1.0, highcut=40.0, order=5):\n",
    "    \"\"\"\n",
    "    Applies minimal preprocessing suitable for models like EEGNet:\n",
    "    Bandpass filtering and channel-wise standardization.\n",
    "\n",
    "    Args:\n",
    "        eeg_data (np.ndarray): Raw EEG data (n_channels, n_timesteps).\n",
    "        fs (float): Original sampling frequency.\n",
    "        lowcut (float): Lower cutoff frequency for bandpass filter (Hz).\n",
    "        highcut (float): Upper cutoff frequency for bandpass filter (Hz).\n",
    "        order (int): Order of the Butterworth filter.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed EEG data (n_channels, n_timesteps).\n",
    "    \"\"\"\n",
    "    n_channels, n_timesteps = eeg_data.shape\n",
    "    processed_data = np.zeros_like(eeg_data)\n",
    "\n",
    "    # 1. Bandpass Filter Design\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    # Ensure frequency bounds are valid\n",
    "    if low <= 0 or high >= 1:\n",
    "         print(f\"Warning: Filter frequencies ({lowcut}Hz, {highcut}Hz) are invalid for Nyquist freq {nyq}Hz. Adjusting or skipping filter.\")\n",
    "         # Option: Skip filtering or adjust bounds\n",
    "         b, a = None, None # Indicate filter skip\n",
    "    else:\n",
    "        try:\n",
    "            b, a = butter(order, [low, high], btype='band')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not design Butterworth filter (order={order}, freqs=[{low}, {high}]). Skipping filter. Error: {e}\")\n",
    "            b, a = None, None\n",
    "\n",
    "\n",
    "    # 2. Apply Filter and Standardize Channel by Channel\n",
    "    for i_ch in range(n_channels):\n",
    "        channel_data = eeg_data[i_ch, :]\n",
    "\n",
    "        # Apply filtering if filter design was successful\n",
    "        if b is not None and a is not None:\n",
    "             try:\n",
    "                 filtered_data = filtfilt(b, a, channel_data)\n",
    "             except Exception as e:\n",
    "                 print(f\"Warning: Filtering failed for channel {i_ch}. Using original data for this channel. Error: {e}\")\n",
    "                 filtered_data = channel_data # Use original if filtering fails\n",
    "        else:\n",
    "             filtered_data = channel_data # Use original if filter wasn't designed\n",
    "\n",
    "        # Standardize (z-score normalization)\n",
    "        mean = np.mean(filtered_data)\n",
    "        std = np.std(filtered_data)\n",
    "        if std > 1e-9: # Avoid division by zero\n",
    "            processed_data[i_ch, :] = (filtered_data - mean) / std\n",
    "        else:\n",
    "            processed_data[i_ch, :] = filtered_data - mean # Only center if std is zero\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vc1xIGdpK0jw"
   },
   "source": [
    "£Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "j5AixcgkscKr"
   },
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EEGNet: compact CNN architecture for EEG-based BCIs.\n",
    "\n",
    "    Implementation based on Lawhern et al. (2018):\n",
    "    \"EEGNet: A Compact Convolutional Neural Network for EEG-based Brain-Computer Interfaces\"\n",
    "\n",
    "    Args:\n",
    "        n_channels (int): Number of EEG channels\n",
    "        n_timesteps (int): Number of timesteps in the EEG signal\n",
    "        num_classes (int): Number of output classes\n",
    "        dropout_rate (float): Dropout probability\n",
    "        F1 (int): Number of temporal filters\n",
    "        D (int): Depth multiplier\n",
    "        F2 (int): Number of pointwise filters\n",
    "        kernel_length (int): Length of temporal kernel\n",
    "        normalize (bool): Whether to use batch normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, n_timesteps, num_classes, dropout_rate=0.5,\n",
    "                 F1=8, D=2, F2=16, kernel_length=64, normalize=True):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.num_classes = num_classes\n",
    "        self.normalize = normalize\n",
    "\n",
    "        # Block 1: Temporal Convolution and Depthwise Spatial Convolution\n",
    "        self.block1 = nn.Sequential(\n",
    "            # Temporal Convolution\n",
    "            nn.Conv2d(1, F1, kernel_size=(1, kernel_length), padding=(0, kernel_length//2), bias=False),\n",
    "            nn.BatchNorm2d(F1) if normalize else nn.Identity(),\n",
    "\n",
    "            # Depthwise Spatial Convolution\n",
    "            nn.Conv2d(F1, F1 * D, kernel_size=(n_channels, 1), groups=F1, bias=False),\n",
    "            nn.BatchNorm2d(F1 * D) if normalize else nn.Identity(),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size=(1, 4)),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Block 2: Separable Convolution\n",
    "        # Separable convolution consists of a depthwise temporal convolution followed by a pointwise convolution\n",
    "        self.block2 = nn.Sequential(\n",
    "            # Depthwise Temporal Convolution\n",
    "            nn.Conv2d(F1 * D, F1 * D, kernel_size=(1, 16), padding=(0, 8), groups=F1 * D, bias=False),\n",
    "            nn.BatchNorm2d(F1 * D) if normalize else nn.Identity(),\n",
    "\n",
    "            # Pointwise Convolution\n",
    "            nn.Conv2d(F1 * D, F2, kernel_size=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(F2) if normalize else nn.Identity(),\n",
    "            nn.ELU(),\n",
    "            nn.AvgPool2d(kernel_size=(1, 8)),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "        # Calculate output feature size for the FC layer\n",
    "        # after temporal convolution: n_timesteps (no change due to padding)\n",
    "        # after first pooling: n_timesteps/4\n",
    "        # after second pooling: n_timesteps/32\n",
    "        self.out_features = F2 * (n_timesteps // 32)\n",
    "\n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(self.out_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (batch_size, n_channels, n_timesteps)\n",
    "        # Reshape for 2D convolution\n",
    "        x = x.unsqueeze(1)  # (batch_size, 1, n_channels, n_timesteps)\n",
    "\n",
    "        # Apply blocks\n",
    "        x = self.block1(x)  # (batch_size, F1*D, 1, n_timesteps/4)\n",
    "        x = self.block2(x)  # (batch_size, F2, 1, n_timesteps/32)\n",
    "\n",
    "        # Flatten for FC layer\n",
    "        x = x.view(x.size(0), -1)  # (batch_size, F2 * (n_timesteps/32))\n",
    "\n",
    "        # Classification\n",
    "        x = self.classifier(x)  # (batch_size, num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define the class weights before using them in the function definition\n",
    "default_class_weight = {\n",
    "    0: 0.7941,   # A\n",
    "    1: 0.8360,   # C\n",
    "    2: 1.8364    # F\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_LE7IO3K0jy"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dqR8q61MK0jy"
   },
   "outputs": [],
   "source": [
    "# --- Modified Generic Training Function with Validation ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Generic function to train and validate a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        val_loader (DataLoader or None): DataLoader for the validation data. If None, validation is skipped.\n",
    "        criterion (nn.Module): The loss function (e.g., nn.CrossEntropyLoss).\n",
    "        optimizer (Optimizer): The optimizer (e.g., optim.Adam).\n",
    "        num_epochs (int): Number of epochs to train for.\n",
    "        device (torch.device): The device to train on (CPU or CUDA).\n",
    "\n",
    "    Returns:\n",
    "        None: Prints training and validation progress information directly.\n",
    "    \"\"\"\n",
    "    model.to(device) # Move model to the designated device\n",
    "    total_train_steps = len(train_loader)\n",
    "    if val_loader:\n",
    "        total_val_steps = len(val_loader)\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n--- Training {model.__class__.__name__} ---\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set the model to training mode\n",
    "        epoch_train_loss = 0.0\n",
    "        train_correct_predictions = 0\n",
    "        train_total_samples = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to the designated device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate training statistics\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total_samples += labels.size(0)\n",
    "            train_correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss and accuracy for the epoch\n",
    "        avg_epoch_train_loss = epoch_train_loss / total_train_steps\n",
    "        epoch_train_accuracy = 100 * train_correct_predictions / train_total_samples\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        if val_loader is not None:\n",
    "            model.eval() # Set the model to evaluation mode\n",
    "            epoch_val_loss = 0.0\n",
    "            val_correct_predictions = 0\n",
    "            val_total_samples = 0\n",
    "\n",
    "            with torch.no_grad(): # Disable gradient calculations during validation\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    # Move data to the designated device\n",
    "                    val_inputs = val_inputs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss_batch = criterion(val_outputs, val_labels)\n",
    "\n",
    "                    # Accumulate validation statistics\n",
    "                    epoch_val_loss += val_loss_batch.item()\n",
    "                    _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "                    val_total_samples += val_labels.size(0)\n",
    "                    val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "            # Calculate average validation loss and accuracy for the epoch\n",
    "            avg_epoch_val_loss = epoch_val_loss / total_val_steps\n",
    "            epoch_val_accuracy = 100 * val_correct_predictions / val_total_samples\n",
    "\n",
    "            # Print combined epoch results\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%, '\n",
    "                  f'Val Loss: {avg_epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%')\n",
    "        else:\n",
    "            # Print only training results if no validation loader is provided\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%')\n",
    "\n",
    "        # Note: model is already set back to train() mode at the start of the next epoch loop iteration\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished Training {model.__class__.__name__}. Total time: {end_time - start_time:.2f} seconds\")\n",
    "    # --- Consider saving the best model based on validation performance ---\n",
    "    # (Logic for tracking best val_accuracy/lowest val_loss and saving model state_dict would go here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ueAyE1l00-lx"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_kd-ONH40-lx"
   },
   "outputs": [],
   "source": [
    "def evaluate_and_compare_models(models, model_names, data_loader, device, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluate multiple trained models on the same dataset and plot ROC curves.\n",
    "\n",
    "    Args:\n",
    "        models (list): List of trained PyTorch models.\n",
    "        model_names (list): List of model names for labeling.\n",
    "        data_loader (DataLoader): DataLoader for validation or test set.\n",
    "        device (torch.device): Device for model execution.\n",
    "        num_classes (int): Total number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        reports (dict): A dictionary of classification report DataFrames per model.\n",
    "    \"\"\"\n",
    "    reports = {}\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for model, name in zip(models, model_names):\n",
    "        if data_loader is None:\n",
    "            print(f\"Skipping {name}: data_loader is None.\")\n",
    "            continue\n",
    "\n",
    "        model.eval()\n",
    "        y_true, y_pred, y_probs = [], [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in data_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "                y_probs.extend(probs)\n",
    "                y_true.extend(y_batch.numpy())\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Print Accuracy\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        print(f\"\\n=== {name} ===\")\n",
    "        print(f\"Accuracy: {acc:.2f}%\")\n",
    "\n",
    "        # Print classification report\n",
    "        report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        display(report_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]])\n",
    "        reports[name] = report_df\n",
    "\n",
    "        # Compute ROC Curve (Micro-average for multiclass)\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n",
    "        y_probs = np.array(y_probs)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_probs.ravel())\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "    # ROC Plot settings\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "    plt.title(\"Micro-Averaged ROC Curves\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z84oEIa-K0jz"
   },
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7GAKWO3K0jz"
   },
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "MY7y9L_fmViM",
    "outputId": "0d8d2c58-6159-43ba-b971-40a51f6e4815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /home/tj2286/alz_det_ML/model-data/fixed_data_aug\n",
      "Test folder exists: True\n",
      "Train folder exists: True\n",
      "Val folder exists: True\n",
      "Labels file exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define paths - everything is in the same directory\n",
    "folder_path_test = 'test'\n",
    "folder_path_train = 'train'\n",
    "folder_path_val = 'validate'\n",
    "file_path_labels = 'labels.json'\n",
    "\n",
    "# Load test data and labels\n",
    "data_val = collect_eeg_data(folder_path_val)\n",
    "data_test = collect_eeg_data(folder_path_test)\n",
    "data_train = collect_eeg_data(folder_path_train)\n",
    "data_labels = read_json_dicts(file_path_labels)\n",
    "\n",
    "# Optional debugging to verify paths\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "print(f\"Test folder exists: {os.path.exists(folder_path_test)}\")\n",
    "print(f\"Train folder exists: {os.path.exists(folder_path_train)}\")\n",
    "print(f\"Val folder exists: {os.path.exists(folder_path_val)}\")\n",
    "print(f\"Labels file exists: {os.path.exists(file_path_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE0CFDcOK0j0"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPn2n7VB0-ly"
   },
   "source": [
    "First, we match the labels with the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0d2jGcEt0-ly"
   },
   "outputs": [],
   "source": [
    "# 1. Strip 'test/' or 'train/' prefix from data_labels.file_name\n",
    "data_labels['file_name'] = data_labels['file_name'].str.replace(r'^(test/|train/)', '', regex=True)\n",
    "\n",
    "# Build a mapping from base filename → label\n",
    "label_map = dict(zip(data_labels['file_name'], data_labels['label']))\n",
    "\n",
    "# 2. Create y_val and filter data_val\n",
    "y_val = {}\n",
    "for fn in list(data_val.keys()):\n",
    "    if fn in label_map:\n",
    "        y_val[fn] = label_map[fn]\n",
    "# keep only matched entries in data_val\n",
    "data_val = {fn: data_val[fn] for fn in y_val}\n",
    "\n",
    "# 3. Create y_test and filter data_test\n",
    "y_test = {}\n",
    "for fn in list(data_test.keys()):\n",
    "    if fn in label_map:\n",
    "        y_test[fn] = label_map[fn]\n",
    "data_test = {fn: data_test[fn] for fn in y_test}\n",
    "\n",
    "# 4. Create y_train and filter data_train, handling augmented filenames\n",
    "def get_base_filename(fn: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove augmentation suffix (_amplitude_scale, _noise, _time_shift) before '.set'.\n",
    "    E.g. 'sub-002_eeg_chunk_0_noise.set' → 'sub-002_eeg_chunk_0.set'\n",
    "    \"\"\"\n",
    "    return re.sub(r'_(amplitude_scale|noise|time_shift)(?=\\.set$)', '', fn)\n",
    "\n",
    "y_train = {}\n",
    "for fn in list(data_train.keys()):\n",
    "    base = get_base_filename(fn)\n",
    "    if base in label_map:\n",
    "        y_train[fn] = label_map[base]\n",
    "# keep only matched entries in data_train\n",
    "data_train = {fn: data_train[fn] for fn in y_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mLs3FGmc0-lz"
   },
   "outputs": [],
   "source": [
    "#pd.Series(y_train.values()).value_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbhDZ4950-lz"
   },
   "source": [
    "Then, we remove the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "v9UWos8w0-lz",
    "outputId": "a2af315b-4515-4bdc-aae1-41f647252d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Applying Minimal Preprocessing WITHOUT Bandpass Filter (Only Standardization) ---\n",
      "Processed 898 test samples; example shape: (19, 1425)\n",
      "Processed 877 validation samples; example shape: (19, 1425)\n",
      "Processed 10644 train samples; example shape: (19, 1425)\n"
     ]
    }
   ],
   "source": [
    "# Set sampling frequency and disable bandpass by using lowcut=0 and highcut=fs\n",
    "fs = 95\n",
    "\n",
    "print(\"\\n--- Applying Minimal Preprocessing WITHOUT Bandpass Filter (Only Standardization) ---\")\n",
    "\n",
    "# Process TEST set\n",
    "X_test_processed = []\n",
    "for fname, eeg in data_test.items():\n",
    "    processed = preprocess_eegnet_minimal(eeg, fs)\n",
    "    X_test_processed.append(processed)\n",
    "print(f\"Processed {len(X_test_processed)} test samples; example shape: {X_test_processed[0].shape}\")\n",
    "\n",
    "# Process VALIDATION set\n",
    "X_val_processed = []\n",
    "for fname, eeg in data_val.items():\n",
    "    processed = preprocess_eegnet_minimal(eeg, fs)\n",
    "    X_val_processed.append(processed)\n",
    "print(f\"Processed {len(X_val_processed)} validation samples; example shape: {X_val_processed[0].shape}\")\n",
    "\n",
    "# Process TRAIN set\n",
    "X_train_processed = []\n",
    "for fname, eeg in data_train.items():\n",
    "    processed = preprocess_eegnet_minimal(eeg, fs)\n",
    "    X_train_processed.append(processed)\n",
    "print(f\"Processed {len(X_train_processed)} train samples; example shape: {X_train_processed[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ye_SRqFZ0-lz"
   },
   "source": [
    "We turn datasets into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "aKdK4W700-lz",
    "outputId": "40dfcfe8-ab8c-478f-c9db-1e49689d735d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converting processed data + labels into tensors & dataloaders ---\n",
      "X_train: torch.Size([10644, 19, 1425]), y_train: torch.Size([10644])\n",
      "X_val  : torch.Size([877, 19, 1425]), y_val  : torch.Size([877])\n",
      "X_test : torch.Size([898, 19, 1425]), y_test : torch.Size([898])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Converting processed data + labels into tensors & dataloaders ---\")\n",
    "\n",
    "# Helper ─ collect labels in the SAME iteration order as the list was built\n",
    "def build_xy_tensors(X_processed_list, y_dict):\n",
    "    \"\"\"\n",
    "    Align X and y by dict iteration order, encode y with LabelEncoder,\n",
    "    and return (X_tensor, y_tensor).\n",
    "    \"\"\"\n",
    "    if not X_processed_list:\n",
    "        raise ValueError(\"The processed X list is empty!\")\n",
    "\n",
    "    # Preserve insertion order of the dict (Python 3.7+ guarantees this)\n",
    "    filenames_order = list(y_dict.keys())\n",
    "\n",
    "    # Sanity check\n",
    "    if len(filenames_order) != len(X_processed_list):\n",
    "        raise RuntimeError(\n",
    "            f\"Mismatch: len(X)={len(X_processed_list)} vs len(y)={len(filenames_order)}. \"\n",
    "            \"Check data alignment.\"\n",
    "        )\n",
    "\n",
    "    # Build y list according to the same order we used when filling X_processed\n",
    "    y_list = [y_dict[fname] for fname in filenames_order]\n",
    "\n",
    "    # --- Label encoding ---\n",
    "    # For consistency across splits, fit once on the union of all labels\n",
    "    global _global_le  # use a single LabelEncoder instance\n",
    "    if '_global_le' not in globals():\n",
    "        _global_le = LabelEncoder()\n",
    "        _global_le.fit(\n",
    "            list(y_train.values()) +\n",
    "            list(y_val.values())   +\n",
    "            list(y_test.values())\n",
    "        )\n",
    "\n",
    "    y_encoded = _global_le.transform(y_list)\n",
    "\n",
    "    # --- Convert to tensors ---\n",
    "    X_np = np.array(X_processed_list)          # shape: (N, C, T)\n",
    "    y_np = np.array(y_encoded, dtype=np.int64) # shape: (N,)\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_np).float()\n",
    "    y_tensor = torch.from_numpy(y_np)          # long by default if dtype not set\n",
    "\n",
    "    return X_tensor, y_tensor\n",
    "\n",
    "# Build tensors for each split\n",
    "X_train_tensor, y_train_tensor = build_xy_tensors(X_train_processed, y_train)\n",
    "\n",
    "# Validation split\n",
    "if X_val_processed:\n",
    "    X_val_tensor, y_val_tensor = build_xy_tensors(X_val_processed, y_val)\n",
    "else:\n",
    "    X_val_tensor = torch.empty((0, *X_train_tensor.shape[1:]), dtype=torch.float32)\n",
    "    y_val_tensor = torch.empty((0,), dtype=torch.long)\n",
    "\n",
    "# Test split\n",
    "if X_test_processed:\n",
    "    X_test_tensor, y_test_tensor = build_xy_tensors(X_test_processed, y_test)\n",
    "else:\n",
    "    X_test_tensor = torch.empty((0, *X_train_tensor.shape[1:]), dtype=torch.float32)\n",
    "    y_test_tensor = torch.empty((0,), dtype=torch.long)\n",
    "\n",
    "# Log shapes\n",
    "print(f\"X_train: {X_train_tensor.shape}, y_train: {y_train_tensor.shape}\")\n",
    "print(f\"X_val  : {X_val_tensor.shape}, y_val  : {y_val_tensor.shape}\")\n",
    "print(f\"X_test : {X_test_tensor.shape}, y_test : {y_test_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UA7YdToc0-l0"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5LyUyR6z0-l0"
   },
   "outputs": [],
   "source": [
    "num_classes =3   # Number of classes=\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1    # Number of epochs (example, usually needs more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhHriG_00-l0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6B80YECK0-l0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterSampler\n",
    "\n",
    "def train_pipeline(\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    X_test_tensor,\n",
    "    y_test_tensor,\n",
    "    *,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=16,\n",
    "    optimizer=\"Adam\",\n",
    "    class_weight=default_class_weight,\n",
    "    dropout_rate=0.5,\n",
    "    num_epochs=100,\n",
    "    weight_decay=1e-4,\n",
    "    F1=8,\n",
    "    D=2,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Full training pipeline for EEGNet model with hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "        X_train_tensor: Training data features\n",
    "        y_train_tensor: Training data labels\n",
    "        X_val_tensor: Validation data features\n",
    "        y_val_tensor: Validation data labels\n",
    "        X_test_tensor: Test data features\n",
    "        y_test_tensor: Test data labels\n",
    "        learning_rate: Learning rate for optimizer\n",
    "        batch_size: Batch size for training\n",
    "        optimizer: Optimizer type (\"Adam\", \"AdamW\", or \"SGD\")\n",
    "        class_weight: Class weights for loss function\n",
    "        dropout_rate: Dropout rate for model\n",
    "        num_epochs: Number of training epochs\n",
    "        weight_decay: L2 regularization strength\n",
    "        F1: Number of temporal filters in EEGNet\n",
    "        D: Depth multiplier in EEGNet\n",
    "        device: Device to run training on\n",
    "        verbose: Whether to print verbose output\n",
    "\n",
    "    Returns:\n",
    "        model: Trained EEGNet model\n",
    "        loaders: Dictionary of data loaders\n",
    "    \"\"\"\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train_tensor, y_train_tensor),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_loader, test_loader = None, None\n",
    "    if X_val_tensor is not None and X_val_tensor.shape[0] > 0:\n",
    "        val_loader = DataLoader(\n",
    "            TensorDataset(X_val_tensor, y_val_tensor),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "    if X_test_tensor is not None and X_test_tensor.shape[0] > 0:\n",
    "        test_loader = DataLoader(\n",
    "            TensorDataset(X_test_tensor, y_test_tensor),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "    # Get dimensions and number of classes\n",
    "    n_channels, n_timesteps = X_train_tensor.shape[1], X_train_tensor.shape[2]\n",
    "    num_classes = len(_global_le.classes_)\n",
    "\n",
    "    # Calculate F2 based on F1 and D\n",
    "    F2 = F1 * D\n",
    "\n",
    "    # Initialize EEGNet model\n",
    "    model = EEGNet(\n",
    "        n_channels=n_channels,\n",
    "        n_timesteps=n_timesteps,\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=dropout_rate,\n",
    "        F1=F1,\n",
    "        D=D,\n",
    "        F2=F2\n",
    "    ).to(device)\n",
    "\n",
    "    # Set up loss function with class weights if provided\n",
    "    if class_weight is not None:\n",
    "        if isinstance(class_weight, dict):\n",
    "            cw_tensor = torch.tensor(\n",
    "                [class_weight.get(i, 1.0) for i in range(num_classes)],\n",
    "                dtype=torch.float32,\n",
    "                device=device\n",
    "            )\n",
    "        else:\n",
    "            cw_tensor = class_weight.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=cw_tensor)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Set up optimizer\n",
    "    opt_name = optimizer.lower()\n",
    "    if opt_name == \"adam\":\n",
    "        optim_obj = optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    elif opt_name == \"adamw\":\n",
    "        optim_obj = optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    elif opt_name == \"sgd\":\n",
    "        optim_obj = optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer {optimizer}\")\n",
    "\n",
    "    # Train the model\n",
    "    train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        optim_obj,\n",
    "        num_epochs,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    return model, {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}\n",
    "\n",
    "\n",
    "# Hyperparameter tuning pipeline\n",
    "def run_hyperparameter_tuning(\n",
    "    X_train_tensor,\n",
    "    y_train_tensor,\n",
    "    X_val_tensor,\n",
    "    y_val_tensor,\n",
    "    X_test_tensor,\n",
    "    y_test_tensor,\n",
    "    n_trials=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Run hyperparameter tuning for EEGNet model.\n",
    "\n",
    "    Args:\n",
    "        X_train_tensor: Training data features\n",
    "        y_train_tensor: Training data labels\n",
    "        X_val_tensor: Validation data features\n",
    "        y_val_tensor: Validation data labels\n",
    "        X_test_tensor: Test data features\n",
    "        y_test_tensor: Test data labels\n",
    "        n_trials: Number of hyperparameter combinations to try\n",
    "\n",
    "    Returns:\n",
    "        best_model: Best trained model\n",
    "        best_config: Best hyperparameter configuration\n",
    "        best_acc: Best validation accuracy\n",
    "    \"\"\"\n",
    "    # Initialize best trackers\n",
    "    best_acc = 0.0\n",
    "    best_config = None\n",
    "    best_model = None\n",
    "    best_model_path = './best_eegnet_model.pt'\n",
    "\n",
    "    # Define search space for hyperparameters\n",
    "    tune_space = {\n",
    "        \"learning_rate\": np.logspace(-4, -2, num=100),  # continuous log space\n",
    "        \"batch_size\": [16, 24, 32, 40, 48, 56, 64],\n",
    "        \"optimizer\": [\"Adam\", \"AdamW\", \"SGD\"],\n",
    "        \"dropout_rate\": np.linspace(0.3, 0.7, num=5),  # [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "        \"num_epochs\": [30],\n",
    "        \"weight_decay\": np.logspace(-3, -1, num=50),\n",
    "        \"F1\": [5, 10, 19, 38, 47, 95],  # Number of temporal filters\n",
    "        \"D\": [2, 3, 4, 5, 6, 7],        # Depth multiplier\n",
    "    }\n",
    "\n",
    "    # Sample random combinations\n",
    "    drawer = list(ParameterSampler(tune_space, n_iter=n_trials, random_state=42))\n",
    "\n",
    "    # Device for training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Loop through each sampled config\n",
    "    for idx, config in enumerate(drawer, 1):\n",
    "        print(f\"\\n=== Configuration {idx}/{n_trials} ===\")\n",
    "        print(config)\n",
    "\n",
    "        # Extract hyperparameters\n",
    "        lr = config[\"learning_rate\"]\n",
    "        bs = config[\"batch_size\"]\n",
    "        opt_name = config[\"optimizer\"]\n",
    "        do_rate = config[\"dropout_rate\"]\n",
    "        epochs = config[\"num_epochs\"]\n",
    "        wd = config[\"weight_decay\"]\n",
    "        f1 = config[\"F1\"]\n",
    "        d = config[\"D\"]\n",
    "\n",
    "        # Train using the pipeline\n",
    "        start_time = time.time()\n",
    "        model, loaders = train_pipeline(\n",
    "            X_train_tensor, y_train_tensor,\n",
    "            X_val_tensor, y_val_tensor,\n",
    "            X_test_tensor, y_test_tensor,\n",
    "            learning_rate=lr,\n",
    "            batch_size=bs,\n",
    "            optimizer=opt_name,\n",
    "            dropout_rate=do_rate,\n",
    "            num_epochs=epochs,\n",
    "            weight_decay=wd,\n",
    "            F1=f1,\n",
    "            D=d,\n",
    "            device=device,\n",
    "            verbose=False\n",
    "        )\n",
    "        train_time = time.time() - start_time\n",
    "        print(f\"Training time: {train_time:.2f} seconds\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_loader = loaders[\"val\"]\n",
    "        if val_loader is not None:\n",
    "            model.eval()\n",
    "            correct, total = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for Xb, yb in val_loader:\n",
    "                    Xb, yb = Xb.to(device), yb.to(device)\n",
    "                    out = model(Xb)\n",
    "                    pred = torch.argmax(out, dim=1)\n",
    "                    correct += (pred == yb).sum().item()\n",
    "                    total += yb.size(0)\n",
    "            val_acc = 100 * correct / total\n",
    "            print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "            # Save model if it achieves a new best accuracy\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                best_config = config\n",
    "                best_model = model\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"--> New best model saved! (acc={best_acc:.2f}%)\")\n",
    "\n",
    "    # Print summary of best result\n",
    "    print(f\"\\n==> Best Validation Accuracy: {best_acc:.2f}%\")\n",
    "    print(\"Best Hyperparameters:\", best_config)\n",
    "    print(f\"Best model saved to: {best_model_path}\")\n",
    "\n",
    "    return best_model, best_config, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9Lwsao7J0-l1",
    "outputId": "171b79ca-0aec-416a-aa81-e4530f596ab5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "=== Configuration 1/10 ===\n",
      "{'weight_decay': np.float64(0.013894954943731374), 'optimizer': 'AdamW', 'num_epochs': 30, 'learning_rate': np.float64(0.008697490026177835), 'dropout_rate': np.float64(0.5), 'batch_size': 48, 'F1': 5, 'D': 7}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 1.0444, Train Acc: 49.64%, Val Loss: 1.9100, Val Acc: 36.83%\n",
      "Epoch [2/30], Train Loss: 0.7953, Train Acc: 63.73%, Val Loss: 2.0504, Val Acc: 31.36%\n",
      "Epoch [3/30], Train Loss: 0.6827, Train Acc: 68.56%, Val Loss: 2.1484, Val Acc: 35.01%\n",
      "Epoch [4/30], Train Loss: 0.6561, Train Acc: 69.76%, Val Loss: 2.0484, Val Acc: 32.04%\n",
      "Epoch [5/30], Train Loss: 0.6190, Train Acc: 70.68%, Val Loss: 2.0142, Val Acc: 35.58%\n",
      "Epoch [6/30], Train Loss: 0.6008, Train Acc: 72.46%, Val Loss: 2.1252, Val Acc: 31.36%\n",
      "Epoch [7/30], Train Loss: 0.5873, Train Acc: 72.49%, Val Loss: 2.2176, Val Acc: 32.16%\n",
      "Epoch [8/30], Train Loss: 0.5827, Train Acc: 72.94%, Val Loss: 1.8409, Val Acc: 38.31%\n",
      "Epoch [9/30], Train Loss: 0.5730, Train Acc: 73.82%, Val Loss: 2.0959, Val Acc: 36.26%\n",
      "Epoch [10/30], Train Loss: 0.5716, Train Acc: 74.13%, Val Loss: 2.3775, Val Acc: 33.30%\n",
      "Epoch [11/30], Train Loss: 0.5560, Train Acc: 74.22%, Val Loss: 2.2439, Val Acc: 35.58%\n",
      "Epoch [12/30], Train Loss: 0.5622, Train Acc: 74.36%, Val Loss: 2.1739, Val Acc: 35.46%\n",
      "Epoch [13/30], Train Loss: 0.5565, Train Acc: 74.22%, Val Loss: 2.5446, Val Acc: 33.18%\n",
      "Epoch [14/30], Train Loss: 0.5378, Train Acc: 75.21%, Val Loss: 2.0291, Val Acc: 36.37%\n",
      "Epoch [15/30], Train Loss: 0.5554, Train Acc: 74.42%, Val Loss: 2.2781, Val Acc: 34.21%\n",
      "Epoch [16/30], Train Loss: 0.5342, Train Acc: 74.92%, Val Loss: 2.2566, Val Acc: 37.74%\n",
      "Epoch [17/30], Train Loss: 0.5355, Train Acc: 74.79%, Val Loss: 2.1332, Val Acc: 36.94%\n",
      "Epoch [18/30], Train Loss: 0.5353, Train Acc: 75.18%, Val Loss: 2.2918, Val Acc: 34.44%\n",
      "Epoch [19/30], Train Loss: 0.5276, Train Acc: 75.91%, Val Loss: 2.4406, Val Acc: 31.13%\n",
      "Epoch [20/30], Train Loss: 0.5405, Train Acc: 74.87%, Val Loss: 2.2032, Val Acc: 34.89%\n",
      "Epoch [21/30], Train Loss: 0.5349, Train Acc: 75.35%, Val Loss: 2.6940, Val Acc: 33.87%\n",
      "Epoch [22/30], Train Loss: 0.5501, Train Acc: 74.84%, Val Loss: 2.4910, Val Acc: 36.60%\n",
      "Epoch [23/30], Train Loss: 0.5332, Train Acc: 75.45%, Val Loss: 2.7494, Val Acc: 31.01%\n",
      "Epoch [24/30], Train Loss: 0.5412, Train Acc: 74.78%, Val Loss: 2.4319, Val Acc: 32.50%\n",
      "Epoch [25/30], Train Loss: 0.5165, Train Acc: 76.19%, Val Loss: 2.4076, Val Acc: 33.87%\n",
      "Epoch [26/30], Train Loss: 0.5223, Train Acc: 75.89%, Val Loss: 2.4229, Val Acc: 35.69%\n",
      "Epoch [27/30], Train Loss: 0.5310, Train Acc: 75.65%, Val Loss: 2.8521, Val Acc: 34.78%\n",
      "Epoch [28/30], Train Loss: 0.5171, Train Acc: 76.32%, Val Loss: 2.7961, Val Acc: 33.41%\n",
      "Epoch [29/30], Train Loss: 0.5269, Train Acc: 75.46%, Val Loss: 2.3208, Val Acc: 38.54%\n",
      "Epoch [30/30], Train Loss: 0.5325, Train Acc: 75.37%, Val Loss: 2.4065, Val Acc: 33.87%\n",
      "Finished Training EEGNet. Total time: 79.97 seconds\n",
      "Training time: 85.46 seconds\n",
      "Validation Accuracy: 33.87%\n",
      "--> New best model saved! (acc=33.87%)\n",
      "\n",
      "=== Configuration 2/10 ===\n",
      "{'weight_decay': np.float64(0.0517947467923121), 'optimizer': 'Adam', 'num_epochs': 30, 'learning_rate': np.float64(0.0022570197196339213), 'dropout_rate': np.float64(0.5), 'batch_size': 32, 'F1': 10, 'D': 6}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 0.9476, Train Acc: 52.21%, Val Loss: 1.4185, Val Acc: 34.78%\n",
      "Epoch [2/30], Train Loss: 0.8391, Train Acc: 59.89%, Val Loss: 1.3230, Val Acc: 34.78%\n",
      "Epoch [3/30], Train Loss: 0.8583, Train Acc: 59.40%, Val Loss: 1.3785, Val Acc: 39.45%\n",
      "Epoch [4/30], Train Loss: 0.8718, Train Acc: 58.21%, Val Loss: 1.4132, Val Acc: 32.61%\n",
      "Epoch [5/30], Train Loss: 0.8763, Train Acc: 57.57%, Val Loss: 1.5750, Val Acc: 37.97%\n",
      "Epoch [6/30], Train Loss: 0.8739, Train Acc: 57.94%, Val Loss: 1.3280, Val Acc: 34.21%\n",
      "Epoch [7/30], Train Loss: 0.8683, Train Acc: 58.61%, Val Loss: 1.4550, Val Acc: 33.98%\n",
      "Epoch [8/30], Train Loss: 0.8782, Train Acc: 57.84%, Val Loss: 1.2972, Val Acc: 35.80%\n",
      "Epoch [9/30], Train Loss: 0.8799, Train Acc: 58.06%, Val Loss: 1.3003, Val Acc: 34.44%\n",
      "Epoch [10/30], Train Loss: 0.8819, Train Acc: 57.60%, Val Loss: 1.3641, Val Acc: 33.41%\n",
      "Epoch [11/30], Train Loss: 0.8820, Train Acc: 57.79%, Val Loss: 1.3876, Val Acc: 35.35%\n",
      "Epoch [12/30], Train Loss: 0.8734, Train Acc: 57.84%, Val Loss: 1.4513, Val Acc: 32.50%\n",
      "Epoch [13/30], Train Loss: 0.8813, Train Acc: 57.53%, Val Loss: 1.3445, Val Acc: 37.63%\n",
      "Epoch [14/30], Train Loss: 0.8767, Train Acc: 58.03%, Val Loss: 1.4098, Val Acc: 35.80%\n",
      "Epoch [15/30], Train Loss: 0.8702, Train Acc: 58.46%, Val Loss: 1.5205, Val Acc: 35.23%\n",
      "Epoch [16/30], Train Loss: 0.8732, Train Acc: 58.11%, Val Loss: 1.4951, Val Acc: 35.35%\n",
      "Epoch [17/30], Train Loss: 0.8804, Train Acc: 57.78%, Val Loss: 1.4160, Val Acc: 34.78%\n",
      "Epoch [18/30], Train Loss: 0.8763, Train Acc: 57.58%, Val Loss: 1.3500, Val Acc: 35.01%\n",
      "Epoch [19/30], Train Loss: 0.8685, Train Acc: 58.72%, Val Loss: 1.6036, Val Acc: 34.66%\n",
      "Epoch [20/30], Train Loss: 0.8767, Train Acc: 57.60%, Val Loss: 1.4186, Val Acc: 33.87%\n",
      "Epoch [21/30], Train Loss: 0.8757, Train Acc: 58.18%, Val Loss: 1.3678, Val Acc: 34.44%\n",
      "Epoch [22/30], Train Loss: 0.8820, Train Acc: 57.59%, Val Loss: 1.2519, Val Acc: 35.23%\n",
      "Epoch [23/30], Train Loss: 0.8693, Train Acc: 58.14%, Val Loss: 1.3149, Val Acc: 36.15%\n",
      "Epoch [24/30], Train Loss: 0.8684, Train Acc: 57.86%, Val Loss: 1.5383, Val Acc: 35.01%\n",
      "Epoch [25/30], Train Loss: 0.8747, Train Acc: 58.39%, Val Loss: 1.4637, Val Acc: 34.09%\n",
      "Epoch [26/30], Train Loss: 0.8714, Train Acc: 58.06%, Val Loss: 1.4194, Val Acc: 33.18%\n",
      "Epoch [27/30], Train Loss: 0.8742, Train Acc: 58.08%, Val Loss: 1.3036, Val Acc: 34.55%\n",
      "Epoch [28/30], Train Loss: 0.8736, Train Acc: 58.11%, Val Loss: 1.4648, Val Acc: 30.56%\n",
      "Epoch [29/30], Train Loss: 0.8724, Train Acc: 58.00%, Val Loss: 1.5356, Val Acc: 37.17%\n",
      "Epoch [30/30], Train Loss: 0.8773, Train Acc: 57.83%, Val Loss: 1.3618, Val Acc: 33.18%\n",
      "Finished Training EEGNet. Total time: 89.32 seconds\n",
      "Training time: 89.33 seconds\n",
      "Validation Accuracy: 33.18%\n",
      "\n",
      "=== Configuration 3/10 ===\n",
      "{'weight_decay': np.float64(0.03906939937054615), 'optimizer': 'AdamW', 'num_epochs': 30, 'learning_rate': np.float64(0.008697490026177835), 'dropout_rate': np.float64(0.6), 'batch_size': 24, 'F1': 47, 'D': 2}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 1.3863, Train Acc: 45.64%, Val Loss: 1.4203, Val Acc: 36.37%\n",
      "Epoch [2/30], Train Loss: 0.8376, Train Acc: 61.09%, Val Loss: 1.5447, Val Acc: 35.12%\n",
      "Epoch [3/30], Train Loss: 0.7643, Train Acc: 65.14%, Val Loss: 1.9288, Val Acc: 33.98%\n",
      "Epoch [4/30], Train Loss: 0.7427, Train Acc: 66.08%, Val Loss: 2.0428, Val Acc: 33.18%\n",
      "Epoch [5/30], Train Loss: 0.7298, Train Acc: 66.99%, Val Loss: 1.5715, Val Acc: 37.97%\n",
      "Epoch [6/30], Train Loss: 0.6972, Train Acc: 68.67%, Val Loss: 2.4886, Val Acc: 32.27%\n",
      "Epoch [7/30], Train Loss: 0.7057, Train Acc: 68.63%, Val Loss: 2.6051, Val Acc: 32.73%\n",
      "Epoch [8/30], Train Loss: 0.7266, Train Acc: 67.76%, Val Loss: 2.1850, Val Acc: 32.27%\n",
      "Epoch [9/30], Train Loss: 0.6940, Train Acc: 68.97%, Val Loss: 2.4576, Val Acc: 31.47%\n",
      "Epoch [10/30], Train Loss: 0.6951, Train Acc: 69.15%, Val Loss: 2.2151, Val Acc: 31.24%\n",
      "Epoch [11/30], Train Loss: 0.6982, Train Acc: 68.60%, Val Loss: 2.4747, Val Acc: 27.59%\n",
      "Epoch [12/30], Train Loss: 0.7115, Train Acc: 69.13%, Val Loss: 1.9807, Val Acc: 33.75%\n",
      "Epoch [13/30], Train Loss: 0.6830, Train Acc: 69.81%, Val Loss: 2.1993, Val Acc: 33.41%\n",
      "Epoch [14/30], Train Loss: 0.6846, Train Acc: 69.60%, Val Loss: 2.9626, Val Acc: 29.76%\n",
      "Epoch [15/30], Train Loss: 0.6941, Train Acc: 69.06%, Val Loss: 2.5770, Val Acc: 33.41%\n",
      "Epoch [16/30], Train Loss: 0.7192, Train Acc: 68.43%, Val Loss: 2.3049, Val Acc: 32.73%\n",
      "Epoch [17/30], Train Loss: 0.6824, Train Acc: 70.06%, Val Loss: 2.4585, Val Acc: 29.42%\n",
      "Epoch [18/30], Train Loss: 0.6926, Train Acc: 69.34%, Val Loss: 2.3047, Val Acc: 33.75%\n",
      "Epoch [19/30], Train Loss: 0.7009, Train Acc: 69.32%, Val Loss: 2.8739, Val Acc: 31.01%\n",
      "Epoch [20/30], Train Loss: 0.6897, Train Acc: 69.23%, Val Loss: 2.1055, Val Acc: 35.58%\n",
      "Epoch [21/30], Train Loss: 0.7086, Train Acc: 68.87%, Val Loss: 2.6075, Val Acc: 37.17%\n",
      "Epoch [22/30], Train Loss: 0.7109, Train Acc: 68.69%, Val Loss: 1.9780, Val Acc: 34.09%\n",
      "Epoch [23/30], Train Loss: 0.6769, Train Acc: 69.67%, Val Loss: 2.2695, Val Acc: 32.16%\n",
      "Epoch [24/30], Train Loss: 0.7027, Train Acc: 69.13%, Val Loss: 1.9359, Val Acc: 35.23%\n",
      "Epoch [25/30], Train Loss: 0.6913, Train Acc: 69.68%, Val Loss: 1.7698, Val Acc: 37.29%\n",
      "Epoch [26/30], Train Loss: 0.6896, Train Acc: 69.38%, Val Loss: 2.2149, Val Acc: 31.58%\n",
      "Epoch [27/30], Train Loss: 0.6894, Train Acc: 69.72%, Val Loss: 2.8879, Val Acc: 28.62%\n",
      "Epoch [28/30], Train Loss: 0.6906, Train Acc: 69.45%, Val Loss: 2.7130, Val Acc: 32.95%\n",
      "Epoch [29/30], Train Loss: 0.6790, Train Acc: 70.35%, Val Loss: 2.4671, Val Acc: 35.46%\n",
      "Epoch [30/30], Train Loss: 0.6770, Train Acc: 69.69%, Val Loss: 2.3684, Val Acc: 32.73%\n",
      "Finished Training EEGNet. Total time: 120.69 seconds\n",
      "Training time: 120.69 seconds\n",
      "Validation Accuracy: 32.73%\n",
      "\n",
      "=== Configuration 4/10 ===\n",
      "{'weight_decay': np.float64(0.029470517025518096), 'optimizer': 'Adam', 'num_epochs': 30, 'learning_rate': np.float64(0.0006734150657750821), 'dropout_rate': np.float64(0.5), 'batch_size': 56, 'F1': 38, 'D': 6}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 0.9851, Train Acc: 49.46%, Val Loss: 1.6510, Val Acc: 35.69%\n",
      "Epoch [2/30], Train Loss: 0.7016, Train Acc: 66.88%, Val Loss: 1.9208, Val Acc: 35.01%\n",
      "Epoch [3/30], Train Loss: 0.6073, Train Acc: 71.43%, Val Loss: 2.1268, Val Acc: 31.36%\n",
      "Epoch [4/30], Train Loss: 0.5852, Train Acc: 72.87%, Val Loss: 1.9456, Val Acc: 35.80%\n",
      "Epoch [5/30], Train Loss: 0.5688, Train Acc: 73.20%, Val Loss: 2.3584, Val Acc: 34.55%\n",
      "Epoch [6/30], Train Loss: 0.5822, Train Acc: 72.22%, Val Loss: 1.8581, Val Acc: 35.01%\n",
      "Epoch [7/30], Train Loss: 0.5641, Train Acc: 73.82%, Val Loss: 1.9969, Val Acc: 34.21%\n",
      "Epoch [8/30], Train Loss: 0.5871, Train Acc: 72.10%, Val Loss: 1.7004, Val Acc: 35.23%\n",
      "Epoch [9/30], Train Loss: 0.6119, Train Acc: 71.45%, Val Loss: 1.6616, Val Acc: 36.03%\n",
      "Epoch [10/30], Train Loss: 0.6202, Train Acc: 70.73%, Val Loss: 1.6265, Val Acc: 36.37%\n",
      "Epoch [11/30], Train Loss: 0.6503, Train Acc: 69.61%, Val Loss: 1.5859, Val Acc: 36.94%\n",
      "Epoch [12/30], Train Loss: 0.6657, Train Acc: 69.02%, Val Loss: 1.6804, Val Acc: 33.41%\n",
      "Epoch [13/30], Train Loss: 0.6785, Train Acc: 68.38%, Val Loss: 1.6189, Val Acc: 37.63%\n",
      "Epoch [14/30], Train Loss: 0.6832, Train Acc: 69.15%, Val Loss: 1.5799, Val Acc: 36.26%\n",
      "Epoch [15/30], Train Loss: 0.6858, Train Acc: 67.71%, Val Loss: 1.6869, Val Acc: 33.30%\n",
      "Epoch [16/30], Train Loss: 0.6991, Train Acc: 67.47%, Val Loss: 1.7012, Val Acc: 34.55%\n",
      "Epoch [17/30], Train Loss: 0.6857, Train Acc: 67.84%, Val Loss: 1.6436, Val Acc: 35.69%\n",
      "Epoch [18/30], Train Loss: 0.7055, Train Acc: 67.08%, Val Loss: 1.5011, Val Acc: 37.29%\n",
      "Epoch [19/30], Train Loss: 0.7014, Train Acc: 66.47%, Val Loss: 1.5491, Val Acc: 37.29%\n",
      "Epoch [20/30], Train Loss: 0.7067, Train Acc: 66.59%, Val Loss: 1.6088, Val Acc: 36.37%\n",
      "Epoch [21/30], Train Loss: 0.6893, Train Acc: 67.78%, Val Loss: 1.4903, Val Acc: 35.69%\n",
      "Epoch [22/30], Train Loss: 0.6949, Train Acc: 67.13%, Val Loss: 1.5558, Val Acc: 36.03%\n",
      "Epoch [23/30], Train Loss: 0.6903, Train Acc: 67.26%, Val Loss: 1.5550, Val Acc: 36.60%\n",
      "Epoch [24/30], Train Loss: 0.6895, Train Acc: 67.37%, Val Loss: 1.6822, Val Acc: 34.89%\n",
      "Epoch [25/30], Train Loss: 0.6935, Train Acc: 67.93%, Val Loss: 1.4403, Val Acc: 37.17%\n",
      "Epoch [26/30], Train Loss: 0.6844, Train Acc: 67.90%, Val Loss: 1.6833, Val Acc: 35.58%\n",
      "Epoch [27/30], Train Loss: 0.6942, Train Acc: 67.25%, Val Loss: 1.7641, Val Acc: 36.60%\n",
      "Epoch [28/30], Train Loss: 0.6954, Train Acc: 68.11%, Val Loss: 1.5208, Val Acc: 37.29%\n",
      "Epoch [29/30], Train Loss: 0.7071, Train Acc: 67.00%, Val Loss: 1.6017, Val Acc: 36.83%\n",
      "Epoch [30/30], Train Loss: 0.6901, Train Acc: 67.83%, Val Loss: 1.4496, Val Acc: 38.54%\n",
      "Finished Training EEGNet. Total time: 179.34 seconds\n",
      "Training time: 179.34 seconds\n",
      "Validation Accuracy: 38.54%\n",
      "--> New best model saved! (acc=38.54%)\n",
      "\n",
      "=== Configuration 5/10 ===\n",
      "{'weight_decay': np.float64(0.005963623316594642), 'optimizer': 'Adam', 'num_epochs': 30, 'learning_rate': np.float64(0.006579332246575682), 'dropout_rate': np.float64(0.39999999999999997), 'batch_size': 32, 'F1': 5, 'D': 5}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 1.0023, Train Acc: 49.46%, Val Loss: 1.6586, Val Acc: 36.15%\n",
      "Epoch [2/30], Train Loss: 0.8200, Train Acc: 61.08%, Val Loss: 1.6775, Val Acc: 31.24%\n",
      "Epoch [3/30], Train Loss: 0.7705, Train Acc: 63.35%, Val Loss: 1.7879, Val Acc: 36.26%\n",
      "Epoch [4/30], Train Loss: 0.7430, Train Acc: 64.54%, Val Loss: 1.8478, Val Acc: 36.49%\n",
      "Epoch [5/30], Train Loss: 0.7505, Train Acc: 64.67%, Val Loss: 1.7540, Val Acc: 35.92%\n",
      "Epoch [6/30], Train Loss: 0.7458, Train Acc: 64.76%, Val Loss: 1.8512, Val Acc: 32.38%\n",
      "Epoch [7/30], Train Loss: 0.7380, Train Acc: 65.47%, Val Loss: 1.7055, Val Acc: 29.65%\n",
      "Epoch [8/30], Train Loss: 0.7208, Train Acc: 65.88%, Val Loss: 1.7704, Val Acc: 36.26%\n",
      "Epoch [9/30], Train Loss: 0.7290, Train Acc: 65.58%, Val Loss: 1.5972, Val Acc: 33.75%\n",
      "Epoch [10/30], Train Loss: 0.7284, Train Acc: 66.36%, Val Loss: 1.7915, Val Acc: 32.61%\n",
      "Epoch [11/30], Train Loss: 0.7282, Train Acc: 66.20%, Val Loss: 1.5744, Val Acc: 33.30%\n",
      "Epoch [12/30], Train Loss: 0.7180, Train Acc: 66.56%, Val Loss: 1.8533, Val Acc: 33.87%\n",
      "Epoch [13/30], Train Loss: 0.7220, Train Acc: 66.35%, Val Loss: 1.6698, Val Acc: 37.06%\n",
      "Epoch [14/30], Train Loss: 0.7110, Train Acc: 66.85%, Val Loss: 1.7648, Val Acc: 34.66%\n",
      "Epoch [15/30], Train Loss: 0.7217, Train Acc: 66.73%, Val Loss: 1.5633, Val Acc: 39.22%\n",
      "Epoch [16/30], Train Loss: 0.7134, Train Acc: 65.93%, Val Loss: 2.0398, Val Acc: 32.61%\n",
      "Epoch [17/30], Train Loss: 0.7092, Train Acc: 67.25%, Val Loss: 1.8555, Val Acc: 30.56%\n",
      "Epoch [18/30], Train Loss: 0.7082, Train Acc: 66.60%, Val Loss: 1.8312, Val Acc: 34.21%\n",
      "Epoch [19/30], Train Loss: 0.7129, Train Acc: 66.58%, Val Loss: 1.5812, Val Acc: 34.09%\n",
      "Epoch [20/30], Train Loss: 0.7072, Train Acc: 67.86%, Val Loss: 2.0001, Val Acc: 30.67%\n",
      "Epoch [21/30], Train Loss: 0.7116, Train Acc: 66.48%, Val Loss: 1.6666, Val Acc: 34.66%\n",
      "Epoch [22/30], Train Loss: 0.7116, Train Acc: 67.24%, Val Loss: 1.7809, Val Acc: 35.35%\n",
      "Epoch [23/30], Train Loss: 0.7044, Train Acc: 66.83%, Val Loss: 1.9388, Val Acc: 31.01%\n",
      "Epoch [24/30], Train Loss: 0.7140, Train Acc: 66.98%, Val Loss: 1.7292, Val Acc: 33.41%\n",
      "Epoch [25/30], Train Loss: 0.7091, Train Acc: 67.02%, Val Loss: 1.8217, Val Acc: 32.61%\n",
      "Epoch [26/30], Train Loss: 0.7198, Train Acc: 66.59%, Val Loss: 1.8753, Val Acc: 33.41%\n",
      "Epoch [27/30], Train Loss: 0.7224, Train Acc: 66.35%, Val Loss: 2.1875, Val Acc: 32.84%\n",
      "Epoch [28/30], Train Loss: 0.7166, Train Acc: 67.09%, Val Loss: 1.6663, Val Acc: 34.78%\n",
      "Epoch [29/30], Train Loss: 0.7179, Train Acc: 66.23%, Val Loss: 1.6364, Val Acc: 30.44%\n",
      "Epoch [30/30], Train Loss: 0.7177, Train Acc: 66.23%, Val Loss: 1.6537, Val Acc: 34.66%\n",
      "Finished Training EEGNet. Total time: 71.83 seconds\n",
      "Training time: 71.83 seconds\n",
      "Validation Accuracy: 34.66%\n",
      "\n",
      "=== Configuration 6/10 ===\n",
      "{'weight_decay': np.float64(0.001), 'optimizer': 'Adam', 'num_epochs': 30, 'learning_rate': np.float64(0.003944206059437656), 'dropout_rate': np.float64(0.39999999999999997), 'batch_size': 16, 'F1': 19, 'D': 5}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 1.0763, Train Acc: 52.95%, Val Loss: 1.9940, Val Acc: 33.98%\n",
      "Epoch [2/30], Train Loss: 0.7668, Train Acc: 66.16%, Val Loss: 2.2967, Val Acc: 32.16%\n",
      "Epoch [3/30], Train Loss: 0.6691, Train Acc: 69.63%, Val Loss: 1.9767, Val Acc: 36.49%\n",
      "Epoch [4/30], Train Loss: 0.6324, Train Acc: 71.53%, Val Loss: 1.9475, Val Acc: 36.15%\n",
      "Epoch [5/30], Train Loss: 0.6093, Train Acc: 72.21%, Val Loss: 2.1350, Val Acc: 33.18%\n",
      "Epoch [6/30], Train Loss: 0.6074, Train Acc: 72.29%, Val Loss: 2.2796, Val Acc: 36.37%\n",
      "Epoch [7/30], Train Loss: 0.5912, Train Acc: 73.28%, Val Loss: 2.3721, Val Acc: 31.93%\n",
      "Epoch [8/30], Train Loss: 0.5835, Train Acc: 73.72%, Val Loss: 2.1052, Val Acc: 37.63%\n",
      "Epoch [9/30], Train Loss: 0.5850, Train Acc: 73.59%, Val Loss: 2.1013, Val Acc: 36.60%\n",
      "Epoch [10/30], Train Loss: 0.5912, Train Acc: 73.71%, Val Loss: 2.2272, Val Acc: 33.18%\n",
      "Epoch [11/30], Train Loss: 0.5890, Train Acc: 73.11%, Val Loss: 1.7797, Val Acc: 37.51%\n",
      "Epoch [12/30], Train Loss: 0.5823, Train Acc: 73.10%, Val Loss: 2.8183, Val Acc: 28.96%\n",
      "Epoch [13/30], Train Loss: 0.5855, Train Acc: 73.74%, Val Loss: 1.9775, Val Acc: 37.29%\n",
      "Epoch [14/30], Train Loss: 0.5831, Train Acc: 73.65%, Val Loss: 2.5873, Val Acc: 35.46%\n",
      "Epoch [15/30], Train Loss: 0.5871, Train Acc: 73.71%, Val Loss: 2.3100, Val Acc: 33.64%\n",
      "Epoch [16/30], Train Loss: 0.5821, Train Acc: 73.69%, Val Loss: 1.9600, Val Acc: 35.69%\n",
      "Epoch [17/30], Train Loss: 0.5797, Train Acc: 73.80%, Val Loss: 1.9489, Val Acc: 35.01%\n",
      "Epoch [18/30], Train Loss: 0.5828, Train Acc: 73.51%, Val Loss: 1.8148, Val Acc: 36.03%\n",
      "Epoch [19/30], Train Loss: 0.5728, Train Acc: 73.76%, Val Loss: 2.7772, Val Acc: 33.18%\n",
      "Epoch [20/30], Train Loss: 0.5778, Train Acc: 74.33%, Val Loss: 1.7860, Val Acc: 36.94%\n",
      "Epoch [21/30], Train Loss: 0.5767, Train Acc: 74.02%, Val Loss: 2.1048, Val Acc: 35.92%\n",
      "Epoch [22/30], Train Loss: 0.5787, Train Acc: 74.36%, Val Loss: 2.6358, Val Acc: 32.73%\n",
      "Epoch [23/30], Train Loss: 0.5797, Train Acc: 73.73%, Val Loss: 1.8305, Val Acc: 35.23%\n",
      "Epoch [24/30], Train Loss: 0.5712, Train Acc: 74.09%, Val Loss: 1.8950, Val Acc: 37.51%\n",
      "Epoch [25/30], Train Loss: 0.5757, Train Acc: 73.86%, Val Loss: 2.0075, Val Acc: 34.89%\n",
      "Epoch [26/30], Train Loss: 0.5653, Train Acc: 74.29%, Val Loss: 2.1805, Val Acc: 36.26%\n",
      "Epoch [27/30], Train Loss: 0.5759, Train Acc: 74.19%, Val Loss: 2.0864, Val Acc: 34.66%\n",
      "Epoch [28/30], Train Loss: 0.5566, Train Acc: 74.48%, Val Loss: 2.1675, Val Acc: 33.98%\n",
      "Epoch [29/30], Train Loss: 0.5821, Train Acc: 73.80%, Val Loss: 1.9290, Val Acc: 39.57%\n",
      "Epoch [30/30], Train Loss: 0.5798, Train Acc: 73.91%, Val Loss: 2.1038, Val Acc: 36.49%\n",
      "Finished Training EEGNet. Total time: 126.91 seconds\n",
      "Training time: 126.92 seconds\n",
      "Validation Accuracy: 36.49%\n",
      "\n",
      "=== Configuration 7/10 ===\n",
      "{'weight_decay': np.float64(0.0012067926406393288), 'optimizer': 'SGD', 'num_epochs': 30, 'learning_rate': np.float64(0.0020565123083486517), 'dropout_rate': np.float64(0.6), 'batch_size': 16, 'F1': 19, 'D': 4}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 1.1456, Train Acc: 43.02%, Val Loss: 1.4275, Val Acc: 36.83%\n",
      "Epoch [2/30], Train Loss: 0.9233, Train Acc: 56.32%, Val Loss: 1.5365, Val Acc: 38.54%\n",
      "Epoch [3/30], Train Loss: 0.7983, Train Acc: 62.43%, Val Loss: 1.8331, Val Acc: 40.59%\n",
      "Epoch [4/30], Train Loss: 0.7242, Train Acc: 66.61%, Val Loss: 1.8694, Val Acc: 33.18%\n",
      "Epoch [5/30], Train Loss: 0.6892, Train Acc: 68.06%, Val Loss: 1.8675, Val Acc: 34.66%\n",
      "Epoch [6/30], Train Loss: 0.6729, Train Acc: 68.41%, Val Loss: 1.7855, Val Acc: 34.89%\n",
      "Epoch [7/30], Train Loss: 0.6508, Train Acc: 70.05%, Val Loss: 1.9424, Val Acc: 35.69%\n",
      "Epoch [8/30], Train Loss: 0.6199, Train Acc: 71.40%, Val Loss: 2.0373, Val Acc: 35.80%\n",
      "Epoch [9/30], Train Loss: 0.6095, Train Acc: 72.10%, Val Loss: 2.0280, Val Acc: 34.09%\n",
      "Epoch [10/30], Train Loss: 0.6091, Train Acc: 71.89%, Val Loss: 2.1316, Val Acc: 34.78%\n",
      "Epoch [11/30], Train Loss: 0.5897, Train Acc: 72.94%, Val Loss: 2.0019, Val Acc: 33.75%\n",
      "Epoch [12/30], Train Loss: 0.5819, Train Acc: 73.23%, Val Loss: 1.8959, Val Acc: 36.15%\n",
      "Epoch [13/30], Train Loss: 0.5688, Train Acc: 74.06%, Val Loss: 2.0809, Val Acc: 36.15%\n",
      "Epoch [14/30], Train Loss: 0.5771, Train Acc: 73.65%, Val Loss: 2.0401, Val Acc: 33.75%\n",
      "Epoch [15/30], Train Loss: 0.5628, Train Acc: 74.35%, Val Loss: 2.1241, Val Acc: 36.83%\n",
      "Epoch [16/30], Train Loss: 0.5576, Train Acc: 74.30%, Val Loss: 2.0239, Val Acc: 36.94%\n",
      "Epoch [17/30], Train Loss: 0.5675, Train Acc: 74.08%, Val Loss: 2.2246, Val Acc: 34.78%\n",
      "Epoch [18/30], Train Loss: 0.5620, Train Acc: 74.47%, Val Loss: 2.0514, Val Acc: 35.58%\n",
      "Epoch [19/30], Train Loss: 0.5452, Train Acc: 74.76%, Val Loss: 1.9691, Val Acc: 37.06%\n",
      "Epoch [20/30], Train Loss: 0.5370, Train Acc: 75.64%, Val Loss: 2.1423, Val Acc: 34.55%\n",
      "Epoch [21/30], Train Loss: 0.5441, Train Acc: 74.78%, Val Loss: 1.9964, Val Acc: 36.72%\n",
      "Epoch [22/30], Train Loss: 0.5471, Train Acc: 74.78%, Val Loss: 2.1251, Val Acc: 35.92%\n",
      "Epoch [23/30], Train Loss: 0.5368, Train Acc: 75.54%, Val Loss: 2.1131, Val Acc: 36.15%\n",
      "Epoch [24/30], Train Loss: 0.5250, Train Acc: 76.25%, Val Loss: 2.1286, Val Acc: 35.23%\n",
      "Epoch [25/30], Train Loss: 0.5297, Train Acc: 75.59%, Val Loss: 2.3657, Val Acc: 33.52%\n",
      "Epoch [26/30], Train Loss: 0.5203, Train Acc: 75.89%, Val Loss: 2.2741, Val Acc: 35.23%\n",
      "Epoch [27/30], Train Loss: 0.5301, Train Acc: 75.82%, Val Loss: 2.3519, Val Acc: 33.41%\n",
      "Epoch [28/30], Train Loss: 0.5205, Train Acc: 75.70%, Val Loss: 2.2124, Val Acc: 34.09%\n",
      "Epoch [29/30], Train Loss: 0.5227, Train Acc: 76.01%, Val Loss: 2.2410, Val Acc: 34.66%\n",
      "Epoch [30/30], Train Loss: 0.5166, Train Acc: 76.60%, Val Loss: 2.2682, Val Acc: 35.46%\n",
      "Finished Training EEGNet. Total time: 118.95 seconds\n",
      "Training time: 118.95 seconds\n",
      "Validation Accuracy: 35.46%\n",
      "\n",
      "=== Configuration 8/10 ===\n",
      "{'weight_decay': np.float64(0.0015998587196060573), 'optimizer': 'Adam', 'num_epochs': 30, 'learning_rate': np.float64(0.00015922827933410923), 'dropout_rate': np.float64(0.5), 'batch_size': 40, 'F1': 38, 'D': 3}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 1.0963, Train Acc: 37.72%, Val Loss: 1.1309, Val Acc: 36.72%\n",
      "Epoch [2/30], Train Loss: 0.9979, Train Acc: 48.52%, Val Loss: 1.1860, Val Acc: 38.77%\n",
      "Epoch [3/30], Train Loss: 0.9085, Train Acc: 55.20%, Val Loss: 1.2948, Val Acc: 40.82%\n",
      "Epoch [4/30], Train Loss: 0.8417, Train Acc: 59.72%, Val Loss: 1.3587, Val Acc: 40.71%\n",
      "Epoch [5/30], Train Loss: 0.7784, Train Acc: 63.12%, Val Loss: 1.4384, Val Acc: 40.71%\n",
      "Epoch [6/30], Train Loss: 0.7352, Train Acc: 65.23%, Val Loss: 1.5025, Val Acc: 39.22%\n",
      "Epoch [7/30], Train Loss: 0.6973, Train Acc: 66.39%, Val Loss: 1.5247, Val Acc: 40.25%\n",
      "Epoch [8/30], Train Loss: 0.6546, Train Acc: 68.48%, Val Loss: 1.5638, Val Acc: 40.14%\n",
      "Epoch [9/30], Train Loss: 0.6360, Train Acc: 69.49%, Val Loss: 1.6848, Val Acc: 39.68%\n",
      "Epoch [10/30], Train Loss: 0.6093, Train Acc: 70.63%, Val Loss: 1.7233, Val Acc: 41.16%\n",
      "Epoch [11/30], Train Loss: 0.5931, Train Acc: 71.37%, Val Loss: 1.7405, Val Acc: 40.71%\n",
      "Epoch [12/30], Train Loss: 0.5755, Train Acc: 72.06%, Val Loss: 1.8569, Val Acc: 39.11%\n",
      "Epoch [13/30], Train Loss: 0.5673, Train Acc: 73.02%, Val Loss: 1.9125, Val Acc: 39.22%\n",
      "Epoch [14/30], Train Loss: 0.5566, Train Acc: 73.49%, Val Loss: 1.9266, Val Acc: 39.11%\n",
      "Epoch [15/30], Train Loss: 0.5358, Train Acc: 74.21%, Val Loss: 2.0658, Val Acc: 37.97%\n",
      "Epoch [16/30], Train Loss: 0.5388, Train Acc: 74.04%, Val Loss: 2.1444, Val Acc: 38.20%\n",
      "Epoch [17/30], Train Loss: 0.5277, Train Acc: 74.43%, Val Loss: 2.0028, Val Acc: 39.11%\n",
      "Epoch [18/30], Train Loss: 0.5213, Train Acc: 75.13%, Val Loss: 2.0484, Val Acc: 40.14%\n",
      "Epoch [19/30], Train Loss: 0.5085, Train Acc: 75.38%, Val Loss: 2.1520, Val Acc: 39.68%\n",
      "Epoch [20/30], Train Loss: 0.5091, Train Acc: 75.70%, Val Loss: 2.1293, Val Acc: 38.20%\n",
      "Epoch [21/30], Train Loss: 0.5051, Train Acc: 75.77%, Val Loss: 2.1428, Val Acc: 38.08%\n",
      "Epoch [22/30], Train Loss: 0.4940, Train Acc: 76.31%, Val Loss: 2.0885, Val Acc: 39.45%\n",
      "Epoch [23/30], Train Loss: 0.4966, Train Acc: 75.78%, Val Loss: 2.2204, Val Acc: 37.86%\n",
      "Epoch [24/30], Train Loss: 0.4909, Train Acc: 76.43%, Val Loss: 2.2598, Val Acc: 37.40%\n",
      "Epoch [25/30], Train Loss: 0.4847, Train Acc: 76.56%, Val Loss: 2.1324, Val Acc: 38.43%\n",
      "Epoch [26/30], Train Loss: 0.4741, Train Acc: 77.28%, Val Loss: 2.4074, Val Acc: 37.97%\n",
      "Epoch [27/30], Train Loss: 0.4715, Train Acc: 77.01%, Val Loss: 2.3763, Val Acc: 37.17%\n",
      "Epoch [28/30], Train Loss: 0.4713, Train Acc: 77.31%, Val Loss: 2.3693, Val Acc: 37.63%\n",
      "Epoch [29/30], Train Loss: 0.4667, Train Acc: 77.69%, Val Loss: 2.4596, Val Acc: 37.51%\n",
      "Epoch [30/30], Train Loss: 0.4621, Train Acc: 78.05%, Val Loss: 2.5696, Val Acc: 37.17%\n",
      "Finished Training EEGNet. Total time: 124.81 seconds\n",
      "Training time: 124.81 seconds\n",
      "Validation Accuracy: 37.17%\n",
      "\n",
      "=== Configuration 9/10 ===\n",
      "{'weight_decay': np.float64(0.004941713361323833), 'optimizer': 'AdamW', 'num_epochs': 30, 'learning_rate': np.float64(0.0002782559402207126), 'dropout_rate': np.float64(0.3), 'batch_size': 48, 'F1': 19, 'D': 6}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 1.0302, Train Acc: 44.48%, Val Loss: 1.2970, Val Acc: 39.22%\n",
      "Epoch [2/30], Train Loss: 0.8028, Train Acc: 62.51%, Val Loss: 1.6210, Val Acc: 37.86%\n",
      "Epoch [3/30], Train Loss: 0.6565, Train Acc: 69.06%, Val Loss: 1.6431, Val Acc: 39.00%\n",
      "Epoch [4/30], Train Loss: 0.5709, Train Acc: 72.95%, Val Loss: 1.9995, Val Acc: 36.03%\n",
      "Epoch [5/30], Train Loss: 0.5258, Train Acc: 74.77%, Val Loss: 2.1435, Val Acc: 35.35%\n",
      "Epoch [6/30], Train Loss: 0.4869, Train Acc: 77.18%, Val Loss: 2.2068, Val Acc: 36.49%\n",
      "Epoch [7/30], Train Loss: 0.4613, Train Acc: 78.07%, Val Loss: 2.4557, Val Acc: 35.12%\n",
      "Epoch [8/30], Train Loss: 0.4402, Train Acc: 78.95%, Val Loss: 2.4544, Val Acc: 35.35%\n",
      "Epoch [9/30], Train Loss: 0.4282, Train Acc: 80.15%, Val Loss: 2.4538, Val Acc: 36.26%\n",
      "Epoch [10/30], Train Loss: 0.4132, Train Acc: 80.25%, Val Loss: 2.8195, Val Acc: 34.32%\n",
      "Epoch [11/30], Train Loss: 0.4045, Train Acc: 80.56%, Val Loss: 2.7698, Val Acc: 34.89%\n",
      "Epoch [12/30], Train Loss: 0.3904, Train Acc: 81.46%, Val Loss: 2.9616, Val Acc: 33.75%\n",
      "Epoch [13/30], Train Loss: 0.3893, Train Acc: 81.64%, Val Loss: 2.8930, Val Acc: 34.89%\n",
      "Epoch [14/30], Train Loss: 0.3766, Train Acc: 82.24%, Val Loss: 2.8662, Val Acc: 34.44%\n",
      "Epoch [15/30], Train Loss: 0.3739, Train Acc: 82.13%, Val Loss: 3.1909, Val Acc: 33.30%\n",
      "Epoch [16/30], Train Loss: 0.3643, Train Acc: 82.79%, Val Loss: 2.9783, Val Acc: 35.12%\n",
      "Epoch [17/30], Train Loss: 0.3583, Train Acc: 82.97%, Val Loss: 3.0942, Val Acc: 34.55%\n",
      "Epoch [18/30], Train Loss: 0.3591, Train Acc: 82.98%, Val Loss: 3.3579, Val Acc: 33.75%\n",
      "Epoch [19/30], Train Loss: 0.3533, Train Acc: 83.04%, Val Loss: 3.3296, Val Acc: 32.84%\n",
      "Epoch [20/30], Train Loss: 0.3438, Train Acc: 83.38%, Val Loss: 3.3679, Val Acc: 33.07%\n",
      "Epoch [21/30], Train Loss: 0.3348, Train Acc: 84.34%, Val Loss: 3.4119, Val Acc: 33.18%\n",
      "Epoch [22/30], Train Loss: 0.3380, Train Acc: 83.93%, Val Loss: 3.5006, Val Acc: 32.61%\n",
      "Epoch [23/30], Train Loss: 0.3382, Train Acc: 84.03%, Val Loss: 3.5844, Val Acc: 33.98%\n",
      "Epoch [24/30], Train Loss: 0.3279, Train Acc: 84.79%, Val Loss: 3.4264, Val Acc: 33.30%\n",
      "Epoch [25/30], Train Loss: 0.3280, Train Acc: 84.87%, Val Loss: 3.6808, Val Acc: 32.95%\n",
      "Epoch [26/30], Train Loss: 0.3219, Train Acc: 85.19%, Val Loss: 3.6448, Val Acc: 32.84%\n",
      "Epoch [27/30], Train Loss: 0.3226, Train Acc: 84.92%, Val Loss: 3.4822, Val Acc: 32.73%\n",
      "Epoch [28/30], Train Loss: 0.3219, Train Acc: 84.81%, Val Loss: 3.6490, Val Acc: 32.73%\n",
      "Epoch [29/30], Train Loss: 0.3120, Train Acc: 85.66%, Val Loss: 3.5593, Val Acc: 32.50%\n",
      "Epoch [30/30], Train Loss: 0.3143, Train Acc: 85.36%, Val Loss: 3.5201, Val Acc: 32.50%\n",
      "Finished Training EEGNet. Total time: 115.20 seconds\n",
      "Training time: 115.20 seconds\n",
      "Validation Accuracy: 32.50%\n",
      "\n",
      "=== Configuration 10/10 ===\n",
      "{'weight_decay': np.float64(0.0019306977288832496), 'optimizer': 'AdamW', 'num_epochs': 30, 'learning_rate': np.float64(0.0003511191734215131), 'dropout_rate': np.float64(0.39999999999999997), 'batch_size': 32, 'F1': 95, 'D': 7}\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 0.9274, Train Acc: 55.70%, Val Loss: 2.2575, Val Acc: 36.49%\n",
      "Epoch [2/30], Train Loss: 0.5845, Train Acc: 72.75%, Val Loss: 2.4922, Val Acc: 36.03%\n",
      "Epoch [3/30], Train Loss: 0.4782, Train Acc: 77.58%, Val Loss: 2.9584, Val Acc: 33.64%\n",
      "Epoch [4/30], Train Loss: 0.4385, Train Acc: 80.04%, Val Loss: 3.3477, Val Acc: 32.95%\n",
      "Epoch [5/30], Train Loss: 0.4006, Train Acc: 81.88%, Val Loss: 3.6082, Val Acc: 32.84%\n",
      "Epoch [6/30], Train Loss: 0.3780, Train Acc: 82.53%, Val Loss: 3.5326, Val Acc: 32.95%\n",
      "Epoch [7/30], Train Loss: 0.3470, Train Acc: 84.54%, Val Loss: 3.8148, Val Acc: 32.84%\n",
      "Epoch [8/30], Train Loss: 0.3237, Train Acc: 85.41%, Val Loss: 4.1777, Val Acc: 34.66%\n",
      "Epoch [9/30], Train Loss: 0.2975, Train Acc: 86.78%, Val Loss: 4.0088, Val Acc: 31.81%\n",
      "Epoch [10/30], Train Loss: 0.2718, Train Acc: 87.97%, Val Loss: 4.0547, Val Acc: 34.44%\n",
      "Epoch [11/30], Train Loss: 0.2503, Train Acc: 89.06%, Val Loss: 4.3795, Val Acc: 35.23%\n",
      "Epoch [12/30], Train Loss: 0.2315, Train Acc: 90.13%, Val Loss: 4.6150, Val Acc: 35.01%\n",
      "Epoch [13/30], Train Loss: 0.2185, Train Acc: 90.33%, Val Loss: 4.7023, Val Acc: 34.66%\n",
      "Epoch [14/30], Train Loss: 0.2098, Train Acc: 91.13%, Val Loss: 5.0629, Val Acc: 33.41%\n",
      "Epoch [15/30], Train Loss: 0.1913, Train Acc: 91.76%, Val Loss: 5.5457, Val Acc: 32.61%\n",
      "Epoch [16/30], Train Loss: 0.1681, Train Acc: 92.62%, Val Loss: 4.9756, Val Acc: 34.66%\n",
      "Epoch [17/30], Train Loss: 0.1675, Train Acc: 93.31%, Val Loss: 5.3571, Val Acc: 33.30%\n",
      "Epoch [18/30], Train Loss: 0.1497, Train Acc: 93.77%, Val Loss: 5.4519, Val Acc: 32.50%\n",
      "Epoch [19/30], Train Loss: 0.1305, Train Acc: 94.43%, Val Loss: 5.5567, Val Acc: 34.21%\n",
      "Epoch [20/30], Train Loss: 0.1324, Train Acc: 94.60%, Val Loss: 5.9174, Val Acc: 34.32%\n",
      "Epoch [21/30], Train Loss: 0.1151, Train Acc: 95.56%, Val Loss: 6.1632, Val Acc: 33.07%\n",
      "Epoch [22/30], Train Loss: 0.1098, Train Acc: 95.57%, Val Loss: 5.4246, Val Acc: 33.87%\n",
      "Epoch [23/30], Train Loss: 0.1031, Train Acc: 95.89%, Val Loss: 5.8143, Val Acc: 33.87%\n",
      "Epoch [24/30], Train Loss: 0.0883, Train Acc: 96.58%, Val Loss: 6.7906, Val Acc: 32.50%\n",
      "Epoch [25/30], Train Loss: 0.0865, Train Acc: 96.59%, Val Loss: 6.0731, Val Acc: 33.41%\n",
      "Epoch [26/30], Train Loss: 0.0832, Train Acc: 96.78%, Val Loss: 6.1423, Val Acc: 32.84%\n",
      "Epoch [27/30], Train Loss: 0.0890, Train Acc: 96.42%, Val Loss: 6.2377, Val Acc: 34.21%\n",
      "Epoch [28/30], Train Loss: 0.0780, Train Acc: 96.91%, Val Loss: 6.9388, Val Acc: 34.66%\n",
      "Epoch [29/30], Train Loss: 0.0779, Train Acc: 97.02%, Val Loss: 7.1887, Val Acc: 31.81%\n",
      "Epoch [30/30], Train Loss: 0.0687, Train Acc: 97.44%, Val Loss: 6.5786, Val Acc: 33.18%\n",
      "Finished Training EEGNet. Total time: 412.24 seconds\n",
      "Training time: 412.25 seconds\n",
      "Validation Accuracy: 33.18%\n",
      "\n",
      "==> Best Validation Accuracy: 38.54%\n",
      "Best Hyperparameters: {'weight_decay': np.float64(0.029470517025518096), 'optimizer': 'Adam', 'num_epochs': 30, 'learning_rate': np.float64(0.0006734150657750821), 'dropout_rate': np.float64(0.5), 'batch_size': 56, 'F1': 38, 'D': 6}\n",
      "Best model saved to: ./best_eegnet_model.pt\n"
     ]
    }
   ],
   "source": [
    "best_model, best_config, best_acc = run_hyperparameter_tuning(\n",
    "    X_train_tensor, y_train_tensor,\n",
    "    X_val_tensor,   y_val_tensor,\n",
    "    X_test_tensor,  y_test_tensor,\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Xwx7OTaP0-l2",
    "outputId": "990c972e-caee-4bc2-b07f-1c94a5ecf191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight_decay': np.float64(0.029470517025518096), 'optimizer': 'Adam', 'num_epochs': 30, 'learning_rate': np.float64(0.0006734150657750821), 'dropout_rate': np.float64(0.5), 'batch_size': 56, 'F1': 38, 'D': 6}\n"
     ]
    }
   ],
   "source": [
    "print(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8DMRkNoP0-l3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/30], Train Loss: 1.0825, Train Acc: 40.76%, Val Loss: 1.1513, Val Acc: 35.01%\n",
      "Epoch [2/30], Train Loss: 1.0043, Train Acc: 49.43%, Val Loss: 1.2136, Val Acc: 35.58%\n",
      "Epoch [3/30], Train Loss: 0.9396, Train Acc: 54.46%, Val Loss: 1.3860, Val Acc: 33.52%\n",
      "Epoch [4/30], Train Loss: 0.8822, Train Acc: 58.39%, Val Loss: 1.5042, Val Acc: 31.13%\n",
      "Epoch [5/30], Train Loss: 0.8303, Train Acc: 60.48%, Val Loss: 1.5879, Val Acc: 32.04%\n",
      "Epoch [6/30], Train Loss: 0.7782, Train Acc: 63.65%, Val Loss: 1.8293, Val Acc: 32.50%\n",
      "Epoch [7/30], Train Loss: 0.7576, Train Acc: 64.64%, Val Loss: 1.7924, Val Acc: 31.47%\n",
      "Epoch [8/30], Train Loss: 0.7307, Train Acc: 65.97%, Val Loss: 1.7561, Val Acc: 33.07%\n",
      "Epoch [9/30], Train Loss: 0.7102, Train Acc: 67.44%, Val Loss: 1.8334, Val Acc: 34.32%\n",
      "Epoch [10/30], Train Loss: 0.6912, Train Acc: 67.47%, Val Loss: 1.9365, Val Acc: 30.90%\n",
      "Epoch [11/30], Train Loss: 0.6812, Train Acc: 67.96%, Val Loss: 1.8648, Val Acc: 33.98%\n",
      "Epoch [12/30], Train Loss: 0.6697, Train Acc: 68.69%, Val Loss: 1.9609, Val Acc: 33.87%\n",
      "Epoch [13/30], Train Loss: 0.6466, Train Acc: 69.64%, Val Loss: 1.9756, Val Acc: 33.18%\n",
      "Epoch [14/30], Train Loss: 0.6402, Train Acc: 69.56%, Val Loss: 2.0121, Val Acc: 33.75%\n",
      "Epoch [15/30], Train Loss: 0.6286, Train Acc: 70.52%, Val Loss: 1.9251, Val Acc: 33.87%\n",
      "Epoch [16/30], Train Loss: 0.6252, Train Acc: 70.59%, Val Loss: 2.1451, Val Acc: 33.87%\n",
      "Epoch [17/30], Train Loss: 0.6244, Train Acc: 70.58%, Val Loss: 2.0937, Val Acc: 33.98%\n",
      "Epoch [18/30], Train Loss: 0.6085, Train Acc: 71.20%, Val Loss: 2.0984, Val Acc: 35.12%\n",
      "Epoch [19/30], Train Loss: 0.6111, Train Acc: 71.40%, Val Loss: 2.1370, Val Acc: 35.35%\n",
      "Epoch [20/30], Train Loss: 0.6122, Train Acc: 71.35%, Val Loss: 2.0558, Val Acc: 34.09%\n",
      "Epoch [21/30], Train Loss: 0.5982, Train Acc: 71.77%, Val Loss: 2.2132, Val Acc: 35.35%\n",
      "Epoch [22/30], Train Loss: 0.6021, Train Acc: 72.41%, Val Loss: 2.2743, Val Acc: 33.18%\n",
      "Epoch [23/30], Train Loss: 0.5963, Train Acc: 72.04%, Val Loss: 2.1520, Val Acc: 34.78%\n",
      "Epoch [24/30], Train Loss: 0.5996, Train Acc: 71.97%, Val Loss: 2.1609, Val Acc: 34.44%\n",
      "Epoch [25/30], Train Loss: 0.5849, Train Acc: 72.07%, Val Loss: 2.1325, Val Acc: 37.17%\n",
      "Epoch [26/30], Train Loss: 0.5808, Train Acc: 72.48%, Val Loss: 2.0760, Val Acc: 35.58%\n",
      "Epoch [27/30], Train Loss: 0.5791, Train Acc: 73.01%, Val Loss: 2.1685, Val Acc: 35.35%\n",
      "Epoch [28/30], Train Loss: 0.5734, Train Acc: 73.12%, Val Loss: 2.1223, Val Acc: 35.92%\n",
      "Epoch [29/30], Train Loss: 0.5792, Train Acc: 72.95%, Val Loss: 2.1469, Val Acc: 36.03%\n",
      "Epoch [30/30], Train Loss: 0.5803, Train Acc: 72.98%, Val Loss: 2.0953, Val Acc: 34.78%\n",
      "Finished Training EEGNet. Total time: 676.35 seconds\n"
     ]
    }
   ],
   "source": [
    "trained_model, loaders = train_pipeline(\n",
    "    X_train_tensor, y_train_tensor,\n",
    "    X_val_tensor,   y_val_tensor,\n",
    "    X_test_tensor,  y_test_tensor,\n",
    "    learning_rate = 0.0006734150657750821,\n",
    "    batch_size    = 56,\n",
    "    optimizer     = \"Adam\",\n",
    "    class_weight  = {0:1.0, 1:1.5, 2:2.0},\n",
    "    dropout_rate  = 0.5,\n",
    "    num_epochs    = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ebzd_b-80-l3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m trained_model, loaders \u001b[38;5;241m=\u001b[39m train_pipeline(\n\u001b[1;32m      2\u001b[0m     X_train_tensor, y_train_tensor,\n\u001b[1;32m      3\u001b[0m     X_val_tensor,   y_val_tensor,\n\u001b[1;32m      4\u001b[0m     X_test_tensor,  y_test_tensor,\n\u001b[0;32m----> 5\u001b[0m     learning_rate \u001b[38;5;241m=\u001b[39m best_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      6\u001b[0m     batch_size    \u001b[38;5;241m=\u001b[39m best_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      7\u001b[0m     optimizer     \u001b[38;5;241m=\u001b[39m best_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m     class_weight  \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m2.0\u001b[39m},\n\u001b[1;32m      9\u001b[0m     dropout_rate  \u001b[38;5;241m=\u001b[39mbest_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     10\u001b[0m     num_epochs    \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_config' is not defined"
     ]
    }
   ],
   "source": [
    "trained_model, loaders = train_pipeline(\n",
    "    X_train_tensor, y_train_tensor,\n",
    "    X_val_tensor,   y_val_tensor,\n",
    "    X_test_tensor,  y_test_tensor,\n",
    "    learning_rate = best_config[\"learning_rate\"],\n",
    "    batch_size    = best_config[\"batch_size\"],\n",
    "    optimizer     = best_config[\"optimizer\"],\n",
    "    class_weight  = {0:1.0, 1:1.5, 2:2.0},\n",
    "    dropout_rate  =best_config[\"dropout_rate\"],\n",
    "    num_epochs    = 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5FKDhrvp0-l4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "def evaluate_eegnet(\n",
    "    model,\n",
    "    data_loader,\n",
    "    device,\n",
    "    class_names=None,\n",
    "    verbose=True,\n",
    "    plot_confusion=True,\n",
    "    plot_roc=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a trained EEGNet model on a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The trained EEGNet model to evaluate\n",
    "    data_loader : DataLoader\n",
    "        DataLoader for the evaluation dataset\n",
    "    device : torch.device\n",
    "        Device to run evaluation on\n",
    "    class_names : list, optional\n",
    "        Names of the classes for better reporting\n",
    "    verbose : bool, default=True\n",
    "        Whether to print evaluation results\n",
    "    plot_confusion : bool, default=True\n",
    "        Whether to plot confusion matrix\n",
    "    plot_roc : bool, default=True\n",
    "        Whether to plot ROC curves\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    if class_names is None and hasattr(data_loader.dataset, 'classes'):\n",
    "        class_names = data_loader.dataset.classes\n",
    "\n",
    "    # Ensure model is in evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize lists to store predictions and ground truth\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_probs = []\n",
    "\n",
    "    # Disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get probabilities and predictions\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "            # Store results\n",
    "            y_true.extend(targets.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "            y_probs.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays for easier handling\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_probs = np.array(y_probs)\n",
    "\n",
    "    # Get number of classes\n",
    "    num_classes = len(np.unique(y_true)) if class_names is None else len(class_names)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Handle cases where some classes might not be present in the test set\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Create classification report\n",
    "    if class_names is not None:\n",
    "        report = classification_report(\n",
    "            y_true, y_pred,\n",
    "            target_names=class_names,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "    else:\n",
    "        report = classification_report(\n",
    "            y_true, y_pred,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "    # Prepare ROC curve data\n",
    "    if num_classes == 2:\n",
    "        # Binary classification\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_probs[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_data = [(fpr, tpr, roc_auc, \"ROC curve\")]\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        roc_data = []\n",
    "        # One-vs-rest ROC curves for each class\n",
    "        for i in range(num_classes):\n",
    "            class_label = class_names[i] if class_names is not None else f\"Class {i}\"\n",
    "            fpr, tpr, _ = roc_curve((y_true == i).astype(int), y_probs[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_data.append((fpr, tpr, roc_auc, f\"ROC curve for {class_label}\"))\n",
    "\n",
    "    # Print results if verbose\n",
    "    if verbose:\n",
    "        print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "        print(f\"Precision: {precision*100:.2f}%\")\n",
    "        print(f\"Recall: {recall*100:.2f}%\")\n",
    "        print(f\"F1 Score: {f1*100:.2f}%\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report_df[[\"precision\", \"recall\", \"f1-score\", \"support\"]])\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    if plot_confusion:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.colorbar()\n",
    "\n",
    "        tick_marks = np.arange(num_classes)\n",
    "        plt.xticks(tick_marks, class_names if class_names else tick_marks)\n",
    "        plt.yticks(tick_marks, class_names if class_names else tick_marks)\n",
    "\n",
    "        # Add text annotations to the confusion matrix\n",
    "        thresh = cm.max() / 2.\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                        horizontalalignment=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.xlabel('Predicted label')\n",
    "        plt.ylabel('True label')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Plot ROC curves\n",
    "    if plot_roc:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for fpr, tpr, roc_auc, label in roc_data:\n",
    "            plt.plot(fpr, tpr, lw=2, label=f'{label} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Prepare results dictionary\n",
    "    results = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm,\n",
    "        'classification_report': report_df,\n",
    "        'roc_data': roc_data,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'y_probs': y_probs\n",
    "    }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QkihA7yL0-l5",
    "outputId": "35eb40b8-d538-4cb8-a877-090254fa877c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      2\u001b[0m     TensorDataset(X_test_tensor, y_test_tensor),\n\u001b[1;32m      3\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m evaluate_eegnet(\n\u001b[0;32m----> 7\u001b[0m     model\u001b[38;5;241m=\u001b[39m trained_model,\n\u001b[1;32m      8\u001b[0m     data_loader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[1;32m      9\u001b[0m     device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trained_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test_tensor, y_test_tensor),\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "evaluate_eegnet(\n",
    "    model= trained_model,\n",
    "    data_loader=test_loader,\n",
    "    device=torch.device(\"cpu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
