{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import mne\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time # To measure time\n",
    "\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import detrend, butter, filtfilt\n",
    "import pywt\n",
    "\n",
    "\n",
    "from scipy.signal import resample\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_float, img_as_ubyte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load subject labels\n",
    "with open(\"model-data/Labels_epochs.json\", \"r\") as f:\n",
    "    subject_labels = json.load(f)\n",
    "\n",
    "def load_data(directory):\n",
    "    \"\"\"\n",
    "    Loads and returns augmented EEG data (X) and corresponding labels (y) from the specified directory.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".npy\"):\n",
    "            # Load the features (EEG data)\n",
    "            X_data = np.load(os.path.join(directory, file))\n",
    "            # Load corresponding label (subject ID matches file naming convention)\n",
    "            subject_id = file.split(\"_\")[0]\n",
    "            label = subject_labels.get(subject_id, None)\n",
    "            if label is not None:\n",
    "                X.append(X_data)\n",
    "                y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Assume fs (sampling frequency) is known. For example, fs = 250 Hz\n",
    "fs = 250.0  # Replace this with the actual sampling frequency\n",
    "\n",
    "# Define your preprocessing function\n",
    "def preprocess_eeg_data(X, fs, lowcut=1.0, highcut=40.0, order=5):\n",
    "    \"\"\"\n",
    "    Applies preprocessing steps to EEG data:\n",
    "    - Bandpass filtering\n",
    "    - Channel-wise standardization (z-score normalization)\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Raw EEG data (n_subjects, n_epochs, n_channels, n_timesteps).\n",
    "        fs (float): Original sampling frequency.\n",
    "        lowcut (float): Lower cutoff frequency for bandpass filter (Hz).\n",
    "        highcut (float): Upper cutoff frequency for bandpass filter (Hz).\n",
    "        order (int): Order of the Butterworth filter.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed EEG data (n_subjects, n_epochs, n_channels, n_timesteps).\n",
    "    \"\"\"\n",
    "    n_subjects, n_epochs, n_channels, n_timesteps = X.shape\n",
    "    processed_data = np.zeros_like(X)\n",
    "\n",
    "    # 1. Bandpass Filter Design\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    if low <= 0 or high >= 1:\n",
    "        print(f\"Warning: Filter frequencies ({lowcut}Hz, {highcut}Hz) are invalid for Nyquist freq {nyq}Hz. Skipping filter.\")\n",
    "        b, a = None, None\n",
    "    else:\n",
    "        try:\n",
    "            b, a = butter(order, [low, high], btype='band')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not design Butterworth filter (order={order}, freqs=[{low}, {high}]). Skipping filter. Error: {e}\")\n",
    "            b, a = None, None\n",
    "\n",
    "    # 2. Apply Filter and Standardize Channel by Channel\n",
    "    for i_subj in range(n_subjects):\n",
    "        for i_epoch in range(n_epochs):\n",
    "            for i_ch in range(n_channels):\n",
    "                channel_data = X[i_subj, i_epoch, i_ch, :]\n",
    "\n",
    "                # Apply bandpass filter if designed\n",
    "                if b is not None and a is not None:\n",
    "                    try:\n",
    "                        filtered_data = filtfilt(b, a, channel_data)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Filtering failed for subject {i_subj}, epoch {i_epoch}, channel {i_ch}. Using original data. Error: {e}\")\n",
    "                        filtered_data = channel_data\n",
    "                else:\n",
    "                    filtered_data = channel_data  # Use original if no filter\n",
    "\n",
    "                # Standardize (z-score normalization)\n",
    "                mean = np.mean(filtered_data)\n",
    "                std = np.std(filtered_data)\n",
    "                if std > 1e-9:\n",
    "                    processed_data[i_subj, i_epoch, i_ch, :] = (filtered_data - mean) / std\n",
    "                else:\n",
    "                    processed_data[i_subj, i_epoch, i_ch, :] = filtered_data - mean  # Only center if std is zero\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Rakhmatulin et al., 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5m9NEKeLspqi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_timesteps, num_classes,\n",
    "                 F1=8, D=2, F2=16, kernel_length=64, dropout_rate=0.5):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.num_classes = num_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F2\n",
    "        self.kernel_length = kernel_length\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Block 1: Temporal Convolution + Depthwise Spatial Convolution\n",
    "        self.conv1 = nn.Conv2d(19, self.F1, (1, self.kernel_length), padding=(0, self.kernel_length // 2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.F1)\n",
    "\n",
    "        # Depthwise Conv: applied to each feature map spatially (across channels)\n",
    "        self.depthwise_conv = nn.Conv2d(self.F1, self.F1 * self.D, (self.n_channels, 1), groups=self.F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F1 * self.D)\n",
    "        \n",
    "        # Pooling (downsampling)\n",
    "        self.pool1 = nn.AvgPool2d((1, 4))  # Downsample the time dimension\n",
    "\n",
    "        # Block 2: Separable Convolution\n",
    "        separable_kernel_length = 16\n",
    "        self.separable_conv = nn.Sequential(\n",
    "            nn.Conv2d(self.F1 * self.D, self.F1 * self.D, (1, separable_kernel_length),\n",
    "                      padding=(0, separable_kernel_length // 2), groups=self.F1 * self.D, bias=False),\n",
    "            nn.Conv2d(self.F1 * self.D, self.F2, (1, 1), bias=False)\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        self.pool2 = nn.AvgPool2d((1, 8))  # Further downsampling\n",
    "\n",
    "        # Placeholder for fully connected layer, to be created dynamically in forward pass\n",
    "        self.fc_out = None\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = F.elu(self.bn2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Block 2\n",
    "        x = self.separable_conv(x)\n",
    "        x = F.elu(self.bn3(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Print the shape before flattening\n",
    "        print(\"Shape before flattening:\", x.shape)\n",
    "\n",
    "        # Dynamically calculate the flatten_size\n",
    "        flatten_size = x.size(1) * x.size(2) * x.size(3)  # Calculate flattened size\n",
    "\n",
    "        # Create the fully connected layer dynamically with the correct flatten size\n",
    "        if self.fc_out is None:  # Only initialize once\n",
    "            self.fc_out = nn.Linear(flatten_size, self.num_classes)\n",
    "\n",
    "        # Flatten the tensor dynamically\n",
    "        x = x.view(x.size(0), -1)  # This flattens the tensor\n",
    "\n",
    "        # Print the shape after flattening\n",
    "        print(\"Shape after flattening:\", x.shape)\n",
    "\n",
    "        # Fully connected layer output\n",
    "        x = self.fc_out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the input data shape before passing to the model in the training loop\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)  # Move model to the designated device\n",
    "    total_train_steps = len(train_loader)\n",
    "    if val_loader:\n",
    "        total_val_steps = len(val_loader)\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n--- Training {model.__class__.__name__} ---\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train()  # Set the model to training mode\n",
    "        epoch_train_loss = 0.0\n",
    "        train_correct_predictions = 0\n",
    "        train_total_samples = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to the designated device\n",
    "            inputs = inputs.to(device)  # Shape: (batch_size, 476, 19, 190)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Reshape inputs from (batch_size, 476, 19, 190) to (batch_size, 19, 190, 476)\n",
    "            inputs = inputs.permute(0, 2, 3, 1)  # This changes the shape to (batch_size, 19, 190, 476)\n",
    "            # Now inputs are (batch_size, 19, 190, 476), which matches the expected input for the model\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)  # Model expects input shape (batch_size, 19, 190)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate training statistics\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total_samples += labels.size(0)\n",
    "            train_correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss and accuracy for the epoch\n",
    "        avg_epoch_train_loss = epoch_train_loss / total_train_steps\n",
    "        epoch_train_accuracy = 100 * train_correct_predictions / train_total_samples\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        if val_loader is not None:\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            epoch_val_loss = 0.0\n",
    "            val_correct_predictions = 0\n",
    "            val_total_samples = 0\n",
    "\n",
    "            with torch.no_grad():  # Disable gradient calculations during validation\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    # Move data to the designated device\n",
    "                    val_inputs = val_inputs.to(device)  # Shape: (batch_size, 476, 19, 190)\n",
    "                    val_labels = val_labels.to(device)\n",
    "\n",
    "                    # Reshape validation inputs from (batch_size, 476, 19, 190) to (batch_size, 19, 190, 476)\n",
    "                    val_inputs = val_inputs.permute(0, 2, 3, 1)  # Shape: (batch_size, 19, 190, 476)\n",
    "\n",
    "                    # Forward pass\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss_batch = criterion(val_outputs, val_labels)\n",
    "\n",
    "                    # Accumulate validation statistics\n",
    "                    epoch_val_loss += val_loss_batch.item()\n",
    "                    _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "                    val_total_samples += val_labels.size(0)\n",
    "                    val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "            # Calculate average validation loss and accuracy for the epoch\n",
    "            avg_epoch_val_loss = epoch_val_loss / total_val_steps\n",
    "            epoch_val_accuracy = 100 * val_correct_predictions / val_total_samples\n",
    "\n",
    "            # Print combined epoch results\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%, '\n",
    "                  f'Val Loss: {avg_epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%')\n",
    "        else:\n",
    "            # Print only training results if no validation loader is provided\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%')\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished Training {model.__class__.__name__}. Total time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (240, 476, 19, 190)\n",
      "Test data shape: (104, 476, 19, 190)\n"
     ]
    }
   ],
   "source": [
    "# train_data, train_labels = load_data(\"model-data/epochs-numpy-corrected/train\")\n",
    "# test_data, test_labels = load_data(\"model-data/epochs-numpy-corrected/test\")\n",
    "\n",
    "# # Check shapes of the loaded data\n",
    "# print(f\"Training data shape: {train_data.shape}\")\n",
    "# print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your raw data\n",
    "train_data, train_labels = load_data(\"model-data/epochs-numpy-corrected/train\")\n",
    "test_data, test_labels = load_data(\"model-data/epochs-numpy-corrected/test\")\n",
    "\n",
    "# Preprocess the EEG data\n",
    "train_data_processed = preprocess_eeg_data(train_data, fs=fs)\n",
    "test_data_processed = preprocess_eeg_data(test_data, fs=fs)\n",
    "\n",
    "# Check shapes of the processed data\n",
    "print(f\"Processed training data shape: {train_data_processed.shape}\")\n",
    "print(f\"Processed test data shape: {test_data_processed.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert processed data to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data_processed, dtype=torch.float32).to(device)  # Shape: (240, 476, 19, 190)\n",
    "test_data_tensor = torch.tensor(test_data_processed, dtype=torch.float32).to(device)    # Shape: (104, 476, 19, 190)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the training labels\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "test_labels_encoded = label_encoder.transform(test_labels)\n",
    "\n",
    "# Convert train labels and test labels to PyTorch tensors\n",
    "train_labels_tensor = torch.tensor(train_labels_encoded, dtype=torch.long).to(device)  # Shape: (240,)\n",
    "test_labels_tensor = torch.tensor(test_labels_encoded, dtype=torch.long).to(device)   # Shape: (104,)\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "batch_size = 32  # Set the batch size as needed\n",
    "\n",
    "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Now you can use the `train_loader` and `test_loader` in your training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the model (assuming you have the number of channels, timesteps, and classes)\n",
    "model3 = EEGNet(n_channels=19, n_timesteps=190, num_classes=len(set(train_labels)))  # Adjust number of classes\n",
    "model3.to(device)  # Move model to GPU or CPU\n",
    "\n",
    "# Loss function (for classification)\n",
    "criterion3 = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Adam with a learning rate of 0.001)\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training EEGNet ---\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([16, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([16, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([8, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([8, 41280])\n",
      "Epoch [1/1], Train Loss: 1.1388, Train Acc: 29.17%, Val Loss: 1.0983, Val Acc: 43.27%\n",
      "Finished Training EEGNet. Total time: 2011.26 seconds\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have train_loader and test_loader set up properly\n",
    "num_epochs = 1  # Set the number of epochs\n",
    "train_model(model3, train_loader, test_loader, criterion3, optimizer3, num_epochs, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model's accuracy on the provided data loader.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained PyTorch model to evaluate.\n",
    "        data_loader (DataLoader): DataLoader for the dataset to evaluate on (e.g., test_loader or val_loader).\n",
    "        device (torch.device): The device to run evaluation on (CPU or CUDA).\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the dataset (in percentage).\n",
    "               Returns 0.0 if the data_loader is empty or None.\n",
    "    \"\"\"\n",
    "    if data_loader is None or len(data_loader) == 0:\n",
    "        print(\"Warning: Evaluation data loader is empty or None. Returning 0.0 accuracy.\")\n",
    "        return 0.0\n",
    "\n",
    "    model.to(device)  # Ensure model is on the correct device\n",
    "    model.eval()      # Set the model to evaluation mode (disables dropout, uses running means/vars for BatchNorm)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations - crucial for evaluation efficiency and correctness\n",
    "        for inputs, labels in data_loader:\n",
    "            # Move data to the designated device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Check if reshaping is necessary (depending on your model's expected input format)\n",
    "            if inputs.ndimension() == 4:  # If input is in the format (batch_size, channels, time_steps, features)\n",
    "                # Reshape or permute as necessary, for example, (batch_size, 19, 190, 476) -> (batch_size, 19, 476, 190)\n",
    "                inputs = inputs.permute(0, 2, 3, 1)  # Adjust this based on your model's expected input format\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability/logit\n",
    "\n",
    "            # Update total samples and correct predictions count\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate final accuracy\n",
    "    accuracy = 100 * correct_predictions / total_samples\n",
    "    print(f\"Evaluation Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([32, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([32, 41280])\n",
      "Shape before flattening: torch.Size([8, 16, 172, 15])\n",
      "Shape after flattening: torch.Size([8, 41280])\n",
      "Evaluation Accuracy: 43.27%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43.26923076923077"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model3, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
