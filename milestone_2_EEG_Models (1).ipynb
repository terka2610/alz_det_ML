{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import mne\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import time # To measure time\n",
    "\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import detrend, butter, filtfilt\n",
    "import pywt\n",
    "\n",
    "\n",
    "from scipy.signal import resample\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_float, img_as_ubyte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load subject labels\n",
    "with open(\"model-data/Labels_epochs.json\", \"r\") as f:\n",
    "    subject_labels = json.load(f)\n",
    "\n",
    "def load_data(directory):\n",
    "    \"\"\"\n",
    "    Loads and returns augmented EEG data (X) and corresponding labels (y) from the specified directory.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".npy\"):\n",
    "            # Load the features (EEG data)\n",
    "            X_data = np.load(os.path.join(directory, file))\n",
    "            # Load corresponding label (subject ID matches file naming convention)\n",
    "            subject_id = file.split(\"_\")[0]\n",
    "            label = subject_labels.get(subject_id, None)\n",
    "            if label is not None:\n",
    "                X.append(X_data)\n",
    "                y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_td_psd_features(epoch_data, fs, power_lambda=0.1, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Calculates the 7 TD-PSD features for a single EEG epoch (single channel).\n",
    "    Based on equations in Amini et al., 2021.\n",
    "\n",
    "    Args:\n",
    "        epoch_data (np.ndarray): 1D numpy array for a single channel epoch.\n",
    "        fs (float): Sampling frequency of the epoch data.\n",
    "        power_lambda (float): Lambda for power transform normalization.\n",
    "        epsilon (float): Small value to prevent log(0) or division by zero.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array containing the 7 log-transformed TD-PSD features.\n",
    "                    Returns NaNs if calculation fails.\n",
    "    \"\"\"\n",
    "    n_samples = len(epoch_data)\n",
    "    if n_samples == 0:\n",
    "        return np.full(7, np.nan)\n",
    "\n",
    "    # Detrend the signal (optional but often good practice)\n",
    "    signal = detrend(epoch_data)\n",
    "\n",
    "    # 1. Calculate Power Spectrum and Moments\n",
    "    try:\n",
    "        # FFT\n",
    "        X = fft(signal)\n",
    "        # Power Spectrum (One-sided, ignoring DC for moments perhaps?)\n",
    "        # Frequencies for moments k: corresponds to frequency bins\n",
    "        freqs = np.fft.fftfreq(n_samples, 1/fs)\n",
    "        # Power spectrum P[k] = |X[k]|^2 / N\n",
    "        P = np.abs(X)**2 / n_samples\n",
    "\n",
    "        # Calculate moments m0, m2, m4\n",
    "        # m_n = sum(f^n * P(f)) df - approximated by sum(k^n * P[k])\n",
    "        # We use the magnitude of frequencies for k, ignore negative freqs?\n",
    "        # Let's use Hjorth parameters definition based on time-domain variance\n",
    "        # m0 = variance(signal) = total power (approx)\n",
    "        m0_bar = np.sum(signal**2) / n_samples # Variance = mean square if mean is zero\n",
    "        if m0_bar < epsilon: m0_bar = epsilon\n",
    "\n",
    "        # m2 = variance of first derivative (activity)\n",
    "        delta_x = np.diff(signal, n=1) * fs # Scale by fs? Hjorth doesn't explicitly scale by fs\n",
    "        m2_bar = np.sum(delta_x**2) / (n_samples -1) # Use n_samples-1?\n",
    "        if m2_bar < epsilon: m2_bar = epsilon\n",
    "\n",
    "\n",
    "        # m4 = variance of second derivative (mobility)\n",
    "        delta2_x = np.diff(signal, n=2) * (fs**2) # Scale by fs^2?\n",
    "        m4_bar = np.sum(delta2_x**2) / (n_samples -2)\n",
    "        if m4_bar < epsilon: m4_bar = epsilon\n",
    "\n",
    "\n",
    "        # Apply power transform (Box-Cox with lambda=0 is log, this is slightly different)\n",
    "        m0 = (m0_bar**power_lambda - 1) / power_lambda if power_lambda != 0 else np.log(m0_bar)\n",
    "        m2 = (m2_bar**power_lambda - 1) / power_lambda if power_lambda != 0 else np.log(m2_bar)\n",
    "        m4 = (m4_bar**power_lambda - 1) / power_lambda if power_lambda != 0 else np.log(m4_bar)\n",
    "\n",
    "        # Ensure moments are positive after transform for log\n",
    "        m0 = max(m0, epsilon)\n",
    "        m2 = max(m2, epsilon)\n",
    "        m4 = max(m4, epsilon)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating moments: {e}\")\n",
    "        return np.full(7, np.nan)\n",
    "\n",
    "    features = np.zeros(7)\n",
    "\n",
    "    # 2. Calculate Features f1, f2, f3\n",
    "    try:\n",
    "        features[0] = np.log(m0) # f1 = log(m0)\n",
    "        # Check for valid subtractions\n",
    "        if m0 <= m2: m0 = m2 + epsilon\n",
    "        if m0 <= m4: m0 = m4 + epsilon\n",
    "        features[1] = np.log(m0 - m2) # f2 = log(m0 - m2)\n",
    "        features[2] = np.log(m0 - m4) # f3 = log(m0 - m4)\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error calculating f1, f2, f3: {e}\")\n",
    "         features[:3] = np.nan\n",
    "\n",
    "\n",
    "    # 3. Calculate Feature f4 (Sparseness)\n",
    "    try:\n",
    "        denominator_sqrt = np.sqrt(max(m0 - m2, epsilon)) * np.sqrt(max(m0 - m4, epsilon))\n",
    "        if denominator_sqrt < epsilon: denominator_sqrt = epsilon\n",
    "        features[3] = np.log(m0 / denominator_sqrt) # f4 = log(S) = log(m0 / sqrt((m0-m2)(m0-m4)))\n",
    "    except Exception as e:\n",
    "         print(f\"Error calculating f4 (Sparseness): {e}\")\n",
    "         features[3] = np.nan\n",
    "\n",
    "    # 4. Calculate Feature f5 (Irregularity Factor - IF)\n",
    "    # IF = (m4/m2) / (m2/m0) based on Hjorth parameters 'complexity'\n",
    "    # Paper formula: sqrt(m4/m2) / sqrt(m2/m0) => m0*m4 / m2^2\n",
    "    try:\n",
    "        if m2 < epsilon: m2 = epsilon\n",
    "        if_val = (m0 * m4) / (m2**2)\n",
    "        features[4] = np.log(max(if_val, epsilon)) # f5 = log(IF)\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating f5 (IF): {e}\")\n",
    "        features[4] = np.nan\n",
    "\n",
    "    # 5. Calculate Feature f6 (Covariance - COV)\n",
    "    # COV = std_dev / mean\n",
    "    try:\n",
    "        mean_val = np.mean(signal)\n",
    "        std_dev_val = np.std(signal)\n",
    "        if abs(mean_val) < epsilon: mean_val = np.sign(mean_val) * epsilon if mean_val != 0 else epsilon\n",
    "        cov_val = std_dev_val / mean_val\n",
    "        features[5] = np.log(max(abs(cov_val), epsilon)) # Log of magnitude? Paper isn't explicit if COV can be negative. Let's take abs.\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating f6 (COV): {e}\")\n",
    "        features[5] = np.nan\n",
    "\n",
    "\n",
    "    # 6. Calculate Feature f7 (Teager Energy Operator - TEO)\n",
    "    try:\n",
    "        # TEO(x[j]) = x[j]^2 - x[j-1]x[j+1]\n",
    "        # Need to handle boundaries (pad or slice)\n",
    "        teo_vals = signal[1:-1]**2 - signal[:-2] * signal[2:]\n",
    "        sum_teo = np.sum(teo_vals)\n",
    "        features[6] = np.log(max(abs(sum_teo), epsilon)) # Log of magnitude? Sum can be negative. Paper isn't explicit. Taking abs.\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating f7 (TEO): {e}\")\n",
    "        features[6] = np.nan\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def preprocess_amini(eeg_data, fs, target_fs=256, target_duration_sec=180, epoch_len=256):\n",
    "    \"\"\"\n",
    "    Preprocesses EEG data according to Amini et al. (2021).\n",
    "    \"\"\"\n",
    "    # 1. Select time window\n",
    "    n_channels, n_timesteps = eeg_data.shape\n",
    "    start_sample = int(60 * fs)\n",
    "    end_sample = start_sample + int(target_duration_sec * fs)\n",
    "    eeg_segment = eeg_data[:, start_sample:end_sample]\n",
    "\n",
    "    # 2. Downsample\n",
    "    if fs != target_fs:\n",
    "        num_samples_resampled = int(eeg_segment.shape[1] * (target_fs / fs))\n",
    "        eeg_resampled = resample(eeg_segment, num_samples_resampled, axis=1)\n",
    "    else:\n",
    "        eeg_resampled = eeg_segment\n",
    "\n",
    "    # 3. Segment into epochs and calculate features\n",
    "    n_channels_res, n_timesteps_res = eeg_resampled.shape\n",
    "    num_epochs = n_timesteps_res // epoch_len\n",
    "    all_channel_features = []\n",
    "\n",
    "    for i_ch in range(n_channels_res):\n",
    "        channel_data = eeg_resampled[i_ch, :]\n",
    "        epoch_features_list = []\n",
    "        for i_epoch in range(num_epochs):\n",
    "            start = i_epoch * epoch_len\n",
    "            end = start + epoch_len\n",
    "            epoch = channel_data[start:end]\n",
    "            features = calculate_td_psd_features(epoch, fs)\n",
    "            if not np.isnan(features).all():\n",
    "                epoch_features_list.append(features)\n",
    "\n",
    "        # Average features across valid epochs for the channel\n",
    "        avg_features = np.nanmean(np.array(epoch_features_list), axis=0) if epoch_features_list else np.full(7, np.nan)\n",
    "        all_channel_features.append(avg_features)\n",
    "\n",
    "    return np.array(all_channel_features)  # shape: (n_channels, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cwt_scales(fs, f_min=1, f_max=100, num_scales=128, wavelet='morl'):\n",
    "    \"\"\"Helper to get CWT scales corresponding to a frequency range.\"\"\"\n",
    "    wname = wavelet\n",
    "    central_freq = pywt.central_frequency(wname)\n",
    "    # Formula: scale = central_frequency * sampling_period / desired_frequency\n",
    "    sampling_period = 1.0 / fs\n",
    "    scales = central_freq * sampling_period / (np.logspace(np.log10(f_min), np.log10(f_max), num_scales))[::-1]\n",
    "    # Frequencies corresponding to these scales (for verification)\n",
    "    # frequencies = pywt.scale2frequency(wname, scales) / sampling_period\n",
    "    # print(f\"Generated {len(scales)} scales for freqs approx {frequencies.min()}-{frequencies.max()} Hz\")\n",
    "    return scales\n",
    "\n",
    "\n",
    "def preprocess_acharya(eeg_data, fs, epoch_sec=5, target_size=(224, 224), wavelet='morl', scales=None):\n",
    "    \"\"\"\n",
    "    Preprocesses raw EEG data according to Acharya et al. (2025).\n",
    "    Segments data, performs CWT for each channel in each epoch, averages CWT magnitudes\n",
    "    across channels, resizes to target image size, and converts to 3-channel image.\n",
    "\n",
    "    Args:\n",
    "        eeg_data (np.ndarray): Raw EEG data (n_channels, n_timesteps).\n",
    "        fs (float): Original sampling frequency.\n",
    "        epoch_sec (int): Duration of each epoch in seconds (default: 5s).\n",
    "        target_size (tuple): Target image size (height, width) (default: (224, 224)).\n",
    "        wavelet (str): Wavelet to use for CWT (default: 'morl').\n",
    "        scales (np.ndarray, optional): Scales to use for CWT. If None, calculated for 1-100 Hz.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: A list of processed images (one for each epoch).\n",
    "                         Each image is a numpy array of shape (height, width, 3).\n",
    "                         Returns empty list if error or no full epochs.\n",
    "    \"\"\"\n",
    "    n_channels, n_timesteps = eeg_data.shape\n",
    "    samples_per_epoch = int(epoch_sec * fs)\n",
    "    num_epochs = n_timesteps // samples_per_epoch\n",
    "\n",
    "    if num_epochs == 0:\n",
    "        print(f\"Error: Data duration ({n_timesteps/fs:.2f}s) is less than epoch duration ({epoch_sec}s). Cannot create epochs.\")\n",
    "        return []\n",
    "\n",
    "    if scales is None:\n",
    "        scales = get_cwt_scales(fs, f_min=1, f_max=100, num_scales=128, wavelet=wavelet) # Example scale selection\n",
    "\n",
    "    processed_images = []\n",
    "\n",
    "    for i_epoch in range(num_epochs):\n",
    "        start = i_epoch * samples_per_epoch\n",
    "        end = start + samples_per_epoch\n",
    "        epoch_data = eeg_data[:, start:end] # Shape: (n_channels, samples_per_epoch)\n",
    "\n",
    "        all_channel_coeffs = []\n",
    "        valid_channel_count = 0\n",
    "        for i_ch in range(n_channels):\n",
    "            try:\n",
    "                # Perform CWT for the channel\n",
    "                coeffs, _ = pywt.cwt(epoch_data[i_ch, :], scales, wavelet)\n",
    "                # Take the magnitude (absolute value) of coefficients\n",
    "                all_channel_coeffs.append(np.abs(coeffs)) # Shape: (num_scales, samples_per_epoch)\n",
    "                valid_channel_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: CWT failed for channel {i_ch}, epoch {i_epoch}: {e}\")\n",
    "                # Optionally append NaNs or zeros if needed, here we just skip\n",
    "\n",
    "        if valid_channel_count == 0:\n",
    "            print(f\"Warning: CWT failed for all channels in epoch {i_epoch}. Skipping epoch.\")\n",
    "            continue\n",
    "\n",
    "        # Average the CWT magnitudes across valid channels\n",
    "        # Shape: (num_scales, samples_per_epoch)\n",
    "        avg_coeffs_mag = np.mean(np.array(all_channel_coeffs), axis=0)\n",
    "\n",
    "        # Normalize the averaged magnitudes (e.g., to 0-1 range for image representation)\n",
    "        min_val = np.min(avg_coeffs_mag)\n",
    "        max_val = np.max(avg_coeffs_mag)\n",
    "        if max_val > min_val:\n",
    "            normalized_coeffs = (avg_coeffs_mag - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            normalized_coeffs = np.zeros_like(avg_coeffs_mag) # Handle case of flat input\n",
    "\n",
    "        # Convert to float image format [0, 1]\n",
    "        image_gray = img_as_float(normalized_coeffs)\n",
    "\n",
    "        # Resize the grayscale image to the target size\n",
    "        try:\n",
    "            # Anti-aliasing is recommended for downsampling\n",
    "            image_resized_gray = resize(image_gray, target_size, anti_aliasing=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error resizing image for epoch {i_epoch}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Convert grayscale to 3-channel image (e.g., by repeating the channel)\n",
    "        image_rgb = np.stack([image_resized_gray]*3, axis=-1) # Shape: (H, W, 3)\n",
    "\n",
    "        # Convert back to uint8 if necessary for some libraries, but float is often fine for PyTorch\n",
    "        # image_rgb_uint8 = img_as_ubyte(image_rgb)\n",
    "\n",
    "        processed_images.append(image_rgb) # Add the processed image for this epoch\n",
    "\n",
    "    return processed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_eegnet_minimal(eeg_data, fs, lowcut=1.0, highcut=40.0, order=5):\n",
    "    \"\"\"\n",
    "    Applies minimal preprocessing suitable for models like EEGNet:\n",
    "    Bandpass filtering and channel-wise standardization.\n",
    "\n",
    "    Args:\n",
    "        eeg_data (np.ndarray): Raw EEG data (n_channels, n_timesteps).\n",
    "        fs (float): Original sampling frequency.\n",
    "        lowcut (float): Lower cutoff frequency for bandpass filter (Hz).\n",
    "        highcut (float): Upper cutoff frequency for bandpass filter (Hz).\n",
    "        order (int): Order of the Butterworth filter.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed EEG data (n_channels, n_timesteps).\n",
    "    \"\"\"\n",
    "    n_channels, n_timesteps = eeg_data.shape\n",
    "    processed_data = np.zeros_like(eeg_data)\n",
    "\n",
    "    # 1. Bandpass Filter Design\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    # Ensure frequency bounds are valid\n",
    "    if low <= 0 or high >= 1:\n",
    "         print(f\"Warning: Filter frequencies ({lowcut}Hz, {highcut}Hz) are invalid for Nyquist freq {nyq}Hz. Adjusting or skipping filter.\")\n",
    "         # Option: Skip filtering or adjust bounds\n",
    "         b, a = None, None # Indicate filter skip\n",
    "    else:\n",
    "        try:\n",
    "            b, a = butter(order, [low, high], btype='band')\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not design Butterworth filter (order={order}, freqs=[{low}, {high}]). Skipping filter. Error: {e}\")\n",
    "            b, a = None, None\n",
    "\n",
    "\n",
    "    # 2. Apply Filter and Standardize Channel by Channel\n",
    "    for i_ch in range(n_channels):\n",
    "        channel_data = eeg_data[i_ch, :]\n",
    "\n",
    "        # Apply filtering if filter design was successful\n",
    "        if b is not None and a is not None:\n",
    "             try:\n",
    "                 filtered_data = filtfilt(b, a, channel_data)\n",
    "             except Exception as e:\n",
    "                 print(f\"Warning: Filtering failed for channel {i_ch}. Using original data for this channel. Error: {e}\")\n",
    "                 filtered_data = channel_data # Use original if filtering fails\n",
    "        else:\n",
    "             filtered_data = channel_data # Use original if filter wasn't designed\n",
    "\n",
    "        # Standardize (z-score normalization)\n",
    "        mean = np.mean(filtered_data)\n",
    "        std = np.std(filtered_data)\n",
    "        if std > 1e-9: # Avoid division by zero\n",
    "            processed_data[i_ch, :] = (filtered_data - mean) / std\n",
    "        else:\n",
    "            processed_data[i_ch, :] = filtered_data - mean # Only center if std is zero\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1： Amini et al., 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "j5AixcgkscKr"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Amini_Adapted_CNN(nn.Module):\n",
    "    def __init__(self, n_channels, n_timesteps, num_classes, dropout_rate=0.5):\n",
    "        super(Amini_Adapted_CNN, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Convolutional Layer(s)\n",
    "        self.conv1_out_channels = 16\n",
    "        self.conv1_kernel_size = 64\n",
    "        self.conv1_stride = 16\n",
    "        self.conv1 = nn.Conv1d(in_channels=n_channels,\n",
    "                               out_channels=self.conv1_out_channels,\n",
    "                               kernel_size=self.conv1_kernel_size,\n",
    "                               stride=self.conv1_stride)\n",
    "        self.bn1 = nn.BatchNorm1d(self.conv1_out_channels)\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        conv1_out_timesteps = (n_timesteps - self.conv1_kernel_size) // self.conv1_stride + 1\n",
    "        self.fc1_input_features = self.conv1_out_channels * conv1_out_timesteps\n",
    "        self.fc1_hidden_units = 128\n",
    "        self.fc2_hidden_units = 64\n",
    "\n",
    "        self.fc1 = nn.Linear(self.fc1_input_features, self.fc1_hidden_units)\n",
    "        self.fc2 = nn.Linear(self.fc1_hidden_units, self.fc2_hidden_units)\n",
    "        self.fc3 = nn.Linear(self.fc2_hidden_units, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))  # (batch_size, conv1_out_channels, conv1_out_timesteps)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: charya et al., 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "76XoNKuqsnln"
   },
   "outputs": [],
   "source": [
    "class ConvNeXt1DBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    1D adaptation of the ConvNeXt block described in Acharya et al. (2025).\n",
    "    Uses Depthwise and Pointwise convolutions adapted for 1D.\n",
    "    r: expansion ratio for inverted bottleneck (typically 4)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_p=0.):\n",
    "        super().__init__()\n",
    "        # Depthwise convolution (applied independently to each channel)\n",
    "        self.dwconv = nn.Conv1d(dim, dim, kernel_size=7, padding=3, groups=dim)\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6) # LayerNorm applied on the channel dimension\n",
    "        # Pointwise convolutions (expand and contract channels)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) # Equivalent to 1x1 Conv for channel expansion\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim) # Equivalent to 1x1 Conv for channel contraction\n",
    "        self.drop_p = drop_p\n",
    "        if self.drop_p > 0.0 :\n",
    "             # LayerScale and DropPath would typically be here in original ConvNeXt,\n",
    "             # Simplified for this adaptation. Using basic dropout.\n",
    "             self.dropout = nn.Dropout(drop_p)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, timesteps)\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "\n",
    "        # LayerNorm needs input (batch_size, seq_len, features)\n",
    "        # Here, seq_len=timesteps, features=channels. So, permute.\n",
    "        x = x.permute(0, 2, 1) # (batch_size, timesteps, channels)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Pointwise Convs (Linear layers operate on the last dimension - channels)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "\n",
    "        # Permute back\n",
    "        x = x.permute(0, 2, 1) # (batch_size, channels, timesteps)\n",
    "\n",
    "        # Add dropout if applicable\n",
    "        if self.drop_p > 0.0:\n",
    "             x = self.dropout(x)\n",
    "\n",
    "        # Residual Connection\n",
    "        x = input + x\n",
    "        return x\n",
    "\n",
    "class EEGConvNeXt_1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D adaptation of the EEGConvNeXt model from Acharya et al. (2025).\n",
    "    Processes raw EEG (n_channels, n_timesteps) directly using 1D ConvNeXt blocks.\n",
    "    This differs significantly from the original paper which uses 2D CWT images.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, n_timesteps, num_classes,\n",
    "                 depths=[1, 1, 2, 1], dims=[96, 192, 384, 768], # Based on Table 3 (R={1,1,2,1}, F starts at 96)\n",
    "                 dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_timesteps = n_timesteps\n",
    "\n",
    "        # --- Stem ---\n",
    "        # Patchify: Use Conv1d to embed channels and reduce timesteps\n",
    "        # Kernel size and stride control the initial downsampling\n",
    "        stem_kernel_size = 4\n",
    "        stem_stride = 4\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv1d(n_channels, dims[0], kernel_size=stem_kernel_size, stride=stem_stride),\n",
    "            # LayerNorm operates on the channel dimension, needs permutation\n",
    "            PermuteLayerNorm(dims[0])\n",
    "        )\n",
    "        current_timesteps = (n_timesteps - stem_kernel_size) // stem_stride + 1\n",
    "\n",
    "        # --- Main Stages ---\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(4): # 4 stages\n",
    "            # Downsampling before stages 2, 3, 4\n",
    "            if i > 0:\n",
    "                downsample_layer = nn.Sequential(\n",
    "                    # LayerNorm before downsampling\n",
    "                    PermuteLayerNorm(dims[i-1]),\n",
    "                    # Conv1d for downsampling (stride=2) and increasing channels\n",
    "                    nn.Conv1d(dims[i-1], dims[i], kernel_size=2, stride=2)\n",
    "                )\n",
    "                self.stages.append(downsample_layer)\n",
    "                current_timesteps = (current_timesteps - 2) // 2 + 1 # Update timesteps after downsampling\n",
    "\n",
    "            # ConvNeXt1D Blocks for the current stage\n",
    "            stage_blocks = nn.Sequential(\n",
    "                *[ConvNeXt1DBlock(dim=dims[i], drop_p=dropout_rate) for _ in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage_blocks)\n",
    "\n",
    "\n",
    "        # --- Output Head ---\n",
    "        self.norm_out = nn.LayerNorm(dims[-1], eps=1e-6) # Final LayerNorm\n",
    "        # Global Average Pooling equivalent for 1D: average over the time dimension\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, n_channels, n_timesteps)\n",
    "        x = self.stem(x)\n",
    "        # x shape: (batch_size, dims[0], current_timesteps after stem)\n",
    "\n",
    "        for stage_module in self.stages:\n",
    "            x = stage_module(x)\n",
    "            # Shape changes after downsampling layers within the loop\n",
    "\n",
    "        # Output head processing\n",
    "        # LayerNorm needs (batch_size, seq_len, features)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, timesteps, channels=dims[-1])\n",
    "        x = self.norm_out(x)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, channels=dims[-1], timesteps)\n",
    "\n",
    "        x = self.avgpool(x) # (batch_size, dims[-1], 1)\n",
    "        x = torch.flatten(x, 1) # (batch_size, dims[-1])\n",
    "        x = self.head(x)      # (batch_size, num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Helper module for LayerNorm after Conv1d\n",
    "class PermuteLayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim, eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, timesteps)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, timesteps, channels)\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, channels, timesteps)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Rakhmatulin et al., 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5m9NEKeLspqi"
   },
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the EEGNet architecture described in Rakhmatulin et al. (2024)\n",
    "    and originally proposed by Lawhern et al. (2018).\n",
    "    Processes raw EEG (n_channels, n_timesteps).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels, n_timesteps, num_classes,\n",
    "                 F1=8, D=2, F2=16, kernel_length=64, dropout_rate=0.5):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.num_classes = num_classes\n",
    "        self.F1 = F1\n",
    "        self.D = D\n",
    "        self.F2 = F2 # Original paper uses F2 = F1 * D\n",
    "        self.kernel_length = kernel_length\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # Block 1: Temporal Convolution + Depthwise Spatial Convolution\n",
    "        # Temporal Conv: kernel (1, kernel_length), output F1 feature maps\n",
    "        # Input shape: (batch_size, 1, n_channels, n_timesteps) - Add a dummy channel dim\n",
    "        self.conv1 = nn.Conv2d(1, self.F1, (1, self.kernel_length), padding=(0, self.kernel_length // 2), bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.F1)\n",
    "        # Depthwise Conv: kernel (n_channels, 1), output F1*D feature maps\n",
    "        # Applied to each F1 map spatially (across channels)\n",
    "        self.depthwise_conv = nn.Conv2d(self.F1, self.F1 * self.D, (self.n_channels, 1), groups=self.F1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.F1 * self.D)\n",
    "        # Pooling\n",
    "        self.pool1 = nn.AvgPool2d((1, 4)) # Downsample time dimension\n",
    "\n",
    "        # Block 2: Separable Convolution\n",
    "        # SeparableConv = Depthwise Conv + Pointwise Conv\n",
    "        # Input shape: (batch_size, F1*D, 1, n_timesteps//4)\n",
    "        separable_kernel_length = 16 # Example kernel size for separable conv\n",
    "        self.separable_conv = nn.Sequential(\n",
    "            # Depthwise part: applies one filter per input channel (F1*D)\n",
    "             nn.Conv2d(self.F1 * self.D, self.F1 * self.D, (1, separable_kernel_length),\n",
    "                       padding=(0, separable_kernel_length // 2), groups=self.F1 * self.D, bias=False),\n",
    "            # Pointwise part: 1x1 conv to mix channels and change depth to F2\n",
    "             nn.Conv2d(self.F1 * self.D, self.F2, (1, 1), bias=False)\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(self.F2)\n",
    "        # Pooling\n",
    "        self.pool2 = nn.AvgPool2d((1, 8)) # Further downsample time dimension\n",
    "\n",
    "        # Calculate Flatten layer input size\n",
    "        # After pool1: T' = T // 4\n",
    "        # After pool2: T'' = T' // 8 = T // 32\n",
    "        # Input to FC: F2 * 1 * (T // 32)\n",
    "        self.flatten_size = self.F2 * (n_timesteps // 32)\n",
    "\n",
    "        # Fully Connected Layer for Classification\n",
    "        self.fc_out = nn.Linear(self.flatten_size, self.num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, n_channels, n_timesteps)\n",
    "\n",
    "        # Add dummy channel dimension for Conv2D layers\n",
    "        x = x.unsqueeze(1)\n",
    "        # x shape: (batch_size, 1, n_channels, n_timesteps)\n",
    "\n",
    "        # Block 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        # Depthwise Conv expects (batch_size, F1, n_channels, time)\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = F.elu(self.bn2(x)) # Activation after BN2\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(x)\n",
    "        # x shape: (batch_size, F1*D, 1, n_timesteps//4)\n",
    "\n",
    "        # Block 2\n",
    "        x = self.separable_conv(x)\n",
    "        x = F.elu(self.bn3(x)) # Activation after BN3\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "        # x shape: (batch_size, F2, 1, n_timesteps//32)\n",
    "\n",
    "        # Flatten for FC layer\n",
    "        x = x.view(x.size(0), -1) # Or x = torch.flatten(x, 1)\n",
    "        # x shape: (batch_size, flatten_size)\n",
    "\n",
    "        # Classification layer\n",
    "        x = self.fc_out(x)\n",
    "        # x shape: (batch_size, num_classes)\n",
    "        # Softmax is applied in the loss function\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modified Generic Training Function with Validation ---\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Generic function to train and validate a PyTorch model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training data.\n",
    "        val_loader (DataLoader or None): DataLoader for the validation data. If None, validation is skipped.\n",
    "        criterion (nn.Module): The loss function (e.g., nn.CrossEntropyLoss).\n",
    "        optimizer (Optimizer): The optimizer (e.g., optim.Adam).\n",
    "        num_epochs (int): Number of epochs to train for.\n",
    "        device (torch.device): The device to train on (CPU or CUDA).\n",
    "\n",
    "    Returns:\n",
    "        None: Prints training and validation progress information directly.\n",
    "    \"\"\"\n",
    "    model.to(device) # Move model to the designated device\n",
    "    total_train_steps = len(train_loader)\n",
    "    if val_loader:\n",
    "        total_val_steps = len(val_loader)\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n--- Training {model.__class__.__name__} ---\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # --- Training Phase ---\n",
    "        model.train() # Set the model to training mode\n",
    "        epoch_train_loss = 0.0\n",
    "        train_correct_predictions = 0\n",
    "        train_total_samples = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to the designated device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate training statistics\n",
    "            epoch_train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total_samples += labels.size(0)\n",
    "            train_correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        # Calculate average training loss and accuracy for the epoch\n",
    "        avg_epoch_train_loss = epoch_train_loss / total_train_steps\n",
    "        epoch_train_accuracy = 100 * train_correct_predictions / train_total_samples\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        if val_loader is not None:\n",
    "            model.eval() # Set the model to evaluation mode\n",
    "            epoch_val_loss = 0.0\n",
    "            val_correct_predictions = 0\n",
    "            val_total_samples = 0\n",
    "\n",
    "            with torch.no_grad(): # Disable gradient calculations during validation\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    # Move data to the designated device\n",
    "                    val_inputs = val_inputs.to(device)\n",
    "                    val_labels = val_labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    val_loss_batch = criterion(val_outputs, val_labels)\n",
    "\n",
    "                    # Accumulate validation statistics\n",
    "                    epoch_val_loss += val_loss_batch.item()\n",
    "                    _, val_predicted = torch.max(val_outputs.data, 1)\n",
    "                    val_total_samples += val_labels.size(0)\n",
    "                    val_correct_predictions += (val_predicted == val_labels).sum().item()\n",
    "\n",
    "            # Calculate average validation loss and accuracy for the epoch\n",
    "            avg_epoch_val_loss = epoch_val_loss / total_val_steps\n",
    "            epoch_val_accuracy = 100 * val_correct_predictions / val_total_samples\n",
    "\n",
    "            # Print combined epoch results\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%, '\n",
    "                  f'Val Loss: {avg_epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.2f}%')\n",
    "        else:\n",
    "            # Print only training results if no validation loader is provided\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.2f}%')\n",
    "\n",
    "        # Note: model is already set back to train() mode at the start of the next epoch loop iteration\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Finished Training {model.__class__.__name__}. Total time: {end_time - start_time:.2f} seconds\")\n",
    "    # --- Consider saving the best model based on validation performance ---\n",
    "    # (Logic for tracking best val_accuracy/lowest val_loss and saving model state_dict would go here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files: ['sub-058_epochs.npy', 'sub-073_X_shift.npy', 'sub-065_X_scale.npy', 'sub-003_X_shift.npy', 'sub-015_X_scale.npy', 'sub-043_X_noise.npy', 'sub-009_X_scale.npy', 'sub-079_X_scale.npy', 'sub-084_X_scale.npy', 'sub-031_X_shift.npy', 'sub-073_epochs.npy', 'sub-044_epochs.npy', 'sub-026_epochs.npy', 'sub-057_X_scale.npy', 'sub-001_X_noise.npy', 'sub-037_epochs.npy', 'sub-024_X_shift.npy', 'sub-055_epochs.npy', 'sub-054_X_shift.npy', 'sub-062_epochs.npy', 'sub-014_X_noise.npy', 'sub-087_X_shift.npy', 'sub-066_X_shift.npy', 'sub-026_X_noise.npy', 'sub-070_X_scale.npy', 'sub-056_X_noise.npy', 'sub-081_epochs.npy', 'sub-048_X_shift.npy', 'sub-078_X_noise.npy', 'sub-085_X_noise.npy', 'sub-069_X_scale.npy', 'sub-082_X_shift.npy', 'sub-011_X_noise.npy', 'sub-047_X_scale.npy', 'sub-051_X_shift.npy', 'sub-061_X_noise.npy', 'sub-021_X_shift.npy', 'sub-050_epochs.npy', 'sub-080_X_noise.npy', 'sub-084_epochs.npy', 'sub-005_X_scale.npy', 'sub-019_epochs.npy', 'sub-013_X_shift.npy', 'sub-023_X_noise.npy', 'sub-075_X_scale.npy', 'sub-063_X_shift.npy', 'sub-010_X_scale.npy', 'sub-046_X_noise.npy', 'sub-060_X_scale.npy', 'sub-036_X_noise.npy', 'sub-008_epochs.npy', 'sub-028_X_shift.npy', 'sub-058_X_shift.npy', 'sub-018_X_noise.npy', 'sub-041_epochs.npy', 'sub-052_X_scale.npy', 'sub-076_epochs.npy', 'sub-044_X_shift.npy', 'sub-022_X_scale.npy', 'sub-074_X_noise.npy', 'sub-034_X_shift.npy', 'sub-081_X_scale.npy', 'sub-082_X_scale.npy', 'sub-069_X_shift.npy', 'sub-059_X_noise.npy', 'sub-019_X_shift.npy', 'sub-047_X_shift.npy', 'sub-046_epochs.npy', 'sub-051_X_scale.npy', 'sub-007_X_noise.npy', 'sub-071_epochs.npy', 'sub-037_X_shift.npy', 'sub-077_X_noise.npy', 'sub-024_epochs.npy', 'sub-005_X_shift.npy', 'sub-013_X_scale.npy', 'sub-045_X_noise.npy', 'sub-075_X_shift.npy', 'sub-038_epochs.npy', 'sub-029_epochs.npy', 'sub-020_X_noise.npy', 'sub-028_X_scale.npy', 'sub-058_X_scale.npy', 'sub-052_X_shift.npy', 'sub-002_epochs.npy', 'sub-035_epochs.npy', 'sub-012_X_noise.npy', 'sub-022_X_shift.npy', 'sub-060_epochs.npy', 'sub-057_epochs.npy', 'sub-062_X_noise.npy', 'sub-034_X_scale.npy', 'sub-081_X_shift.npy', 'sub-086_epochs.npy', 'sub-086_X_noise.npy', 'sub-025_X_noise.npy', 'sub-079_epochs.npy', 'sub-009_X_shift.npy', 'sub-084_X_shift.npy', 'sub-039_X_noise.npy', 'sub-067_X_noise.npy', 'sub-031_X_scale.npy', 'sub-017_X_noise.npy', 'sub-041_X_scale.npy', 'sub-057_X_shift.npy', 'sub-052_epochs.npy', 'sub-065_epochs.npy', 'sub-024_X_scale.npy', 'sub-074_epochs.npy', 'sub-072_X_noise.npy', 'sub-043_epochs.npy', 'sub-032_X_shift.npy', 'sub-016_epochs.npy', 'sub-002_X_noise.npy', 'sub-021_epochs.npy', 'sub-042_X_shift.npy', 'sub-066_X_scale.npy', 'sub-030_X_noise.npy', 'sub-070_X_shift.npy', 'sub-068_epochs.npy', 'sub-016_X_scale.npy', 'sub-048_X_scale.npy', 'sub-035_X_shift.npy', 'sub-004_epochs.npy', 'sub-075_X_noise.npy', 'sub-023_X_scale.npy', 'sub-033_epochs.npy', 'sub-005_X_noise.npy', 'sub-066_epochs.npy', 'sub-077_X_shift.npy', 'sub-018_epochs.npy', 'sub-047_X_noise.npy', 'sub-085_epochs.npy', 'sub-059_X_shift.npy', 'sub-069_X_noise.npy', 'sub-029_X_shift.npy', 'sub-081_X_noise.npy', 'sub-062_X_shift.npy', 'sub-074_X_scale.npy', 'sub-012_X_shift.npy', 'sub-009_epochs.npy', 'sub-052_X_noise.npy', 'sub-018_X_scale.npy', 'sub-083_X_shift.npy', 'sub-068_X_scale.npy', 'sub-088_epochs.npy', 'sub-040_epochs.npy', 'sub-020_X_shift.npy', 'sub-077_epochs.npy', 'sub-060_X_noise.npy', 'sub-050_X_shift.npy', 'sub-015_epochs.npy', 'sub-046_X_scale.npy', 'sub-010_X_noise.npy', 'sub-059_epochs.npy', 'sub-001_X_scale.npy', 'sub-057_X_noise.npy', 'sub-017_X_shift.npy', 'sub-071_X_scale.npy', 'sub-027_X_noise.npy', 'sub-067_X_shift.npy', 'sub-084_X_noise.npy', 'sub-079_X_noise.npy', 'sub-049_X_shift.npy', 'sub-043_X_scale.npy', 'sub-045_epochs.npy', 'sub-033_X_scale.npy', 'sub-065_X_noise.npy', 'sub-010_epochs.npy', 'sub-025_X_shift.npy', 'sub-027_epochs.npy', 'sub-086_X_shift.npy', 'sub-085_X_scale.npy', 'sub-078_X_scale.npy', 'sub-008_X_scale.npy', 'sub-036_epochs.npy', 'sub-056_X_scale.npy', 'sub-001_epochs.npy', 'sub-040_X_shift.npy', 'sub-070_X_noise.npy', 'sub-054_epochs.npy', 'sub-026_X_scale.npy', 'sub-063_epochs.npy', 'sub-080_epochs.npy', 'sub-042_X_noise.npy', 'sub-002_X_shift.npy', 'sub-032_X_noise.npy', 'sub-064_X_scale.npy', 'sub-072_X_shift.npy', 'sub-001_X_shift.npy', 'sub-017_X_scale.npy', 'sub-071_X_shift.npy', 'sub-031_X_noise.npy', 'sub-067_X_scale.npy', 'sub-039_X_scale.npy', 'sub-087_epochs.npy', 'sub-031_epochs.npy', 'sub-003_X_noise.npy', 'sub-055_X_scale.npy', 'sub-006_epochs.npy', 'sub-053_epochs.npy', 'sub-064_epochs.npy', 'sub-025_X_scale.npy', 'sub-086_X_scale.npy', 'sub-085_X_shift.npy', 'sub-048_X_noise.npy', 'sub-075_epochs.npy', 'sub-042_epochs.npy', 'sub-056_X_shift.npy', 'sub-040_X_scale.npy', 'sub-016_X_noise.npy', 'sub-020_epochs.npy', 'sub-026_X_shift.npy', 'sub-030_X_scale.npy', 'sub-066_X_noise.npy', 'sub-087_X_noise.npy', 'sub-002_X_scale.npy', 'sub-064_X_shift.npy', 'sub-072_X_scale.npy', 'sub-024_X_noise.npy', 'sub-047_epochs.npy', 'sub-070_epochs.npy', 'sub-023_X_shift.npy', 'sub-013_X_noise.npy', 'sub-025_epochs.npy', 'sub-012_epochs.npy', 'sub-053_X_shift.npy', 'sub-080_X_shift.npy', 'sub-021_X_noise.npy', 'sub-061_X_shift.npy', 'sub-039_epochs.npy', 'sub-007_X_scale.npy', 'sub-011_X_shift.npy', 'sub-029_X_scale.npy', 'sub-082_epochs.npy', 'sub-034_X_noise.npy', 'sub-062_X_scale.npy', 'sub-028_epochs.npy', 'sub-044_X_noise.npy', 'sub-004_X_shift.npy', 'sub-058_X_noise.npy', 'sub-003_epochs.npy', 'sub-036_X_shift.npy', 'sub-006_X_noise.npy', 'sub-056_epochs.npy', 'sub-050_X_scale.npy']\n",
      "Test files: ['sub-033_X_noise.npy', 'sub-027_X_scale.npy', 'sub-071_X_noise.npy', 'sub-041_X_shift.npy', 'sub-011_epochs.npy', 'sub-064_X_noise.npy', 'sub-032_X_scale.npy', 'sub-042_X_scale.npy', 'sub-016_X_shift.npy', 'sub-049_epochs.npy', 'sub-008_X_noise.npy', 'sub-038_X_shift.npy', 'sub-019_X_scale.npy', 'sub-005_epochs.npy', 'sub-032_epochs.npy', 'sub-037_X_scale.npy', 'sub-067_epochs.npy', 'sub-053_X_noise.npy', 'sub-088_X_scale.npy', 'sub-006_X_shift.npy', 'sub-076_X_shift.npy', 'sub-068_X_noise.npy', 'sub-004_X_noise.npy', 'sub-023_epochs.npy', 'sub-014_epochs.npy', 'sub-029_X_noise.npy', 'sub-021_X_scale.npy', 'sub-013_epochs.npy', 'sub-063_X_scale.npy', 'sub-035_X_noise.npy', 'sub-088_X_shift.npy', 'sub-010_X_shift.npy', 'sub-050_X_noise.npy', 'sub-006_X_scale.npy', 'sub-060_X_shift.npy', 'sub-076_X_scale.npy', 'sub-083_epochs.npy', 'sub-083_X_noise.npy', 'sub-044_X_scale.npy', 'sub-073_X_scale.npy', 'sub-065_X_shift.npy', 'sub-055_X_noise.npy', 'sub-003_X_scale.npy', 'sub-015_X_shift.npy', 'sub-049_X_noise.npy', 'sub-079_X_shift.npy', 'sub-027_X_shift.npy', 'sub-030_epochs.npy', 'sub-007_epochs.npy', 'sub-054_X_scale.npy', 'sub-087_X_scale.npy', 'sub-040_X_noise.npy', 'sub-038_X_scale.npy', 'sub-088_X_noise.npy', 'sub-045_X_shift.npy', 'sub-053_X_scale.npy', 'sub-051_epochs.npy', 'sub-080_X_scale.npy', 'sub-037_X_noise.npy', 'sub-061_X_scale.npy', 'sub-007_X_shift.npy', 'sub-011_X_scale.npy', 'sub-019_X_noise.npy', 'sub-022_X_noise.npy', 'sub-004_X_scale.npy', 'sub-036_X_scale.npy', 'sub-022_epochs.npy', 'sub-039_X_shift.npy', 'sub-009_X_noise.npy', 'sub-015_X_noise.npy', 'sub-072_epochs.npy', 'sub-055_X_shift.npy', 'sub-030_X_shift.npy', 'sub-014_X_scale.npy', 'sub-048_epochs.npy', 'sub-041_X_noise.npy', 'sub-078_epochs.npy', 'sub-049_X_scale.npy', 'sub-043_X_shift.npy', 'sub-033_X_shift.npy', 'sub-073_X_noise.npy', 'sub-038_X_noise.npy', 'sub-078_X_shift.npy', 'sub-008_X_shift.npy', 'sub-017_epochs.npy', 'sub-014_X_shift.npy', 'sub-069_epochs.npy', 'sub-054_X_noise.npy', 'sub-035_X_scale.npy', 'sub-063_X_noise.npy', 'sub-045_X_scale.npy', 'sub-077_X_scale.npy', 'sub-051_X_noise.npy', 'sub-059_X_scale.npy', 'sub-082_X_noise.npy', 'sub-074_X_shift.npy', 'sub-012_X_scale.npy', 'sub-018_X_shift.npy', 'sub-068_X_shift.npy', 'sub-083_X_scale.npy', 'sub-028_X_noise.npy', 'sub-076_X_noise.npy', 'sub-034_epochs.npy', 'sub-020_X_scale.npy', 'sub-061_epochs.npy', 'sub-046_X_shift.npy']\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = load_data(\"model-data/epochs-numpy/train\")\n",
    "test_data, test_labels = load_data(\"model-data/epochs-numpy/test\")\n",
    "\n",
    "# Check shapes of the loaded data\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n\u001b[32m     14\u001b[39m batch_size = \u001b[32m32\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m train_loader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Junior_Spring/ml/alz_det_ML/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:383\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    385\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Junior_Spring/ml/alz_det_ML/venv/lib/python3.12/site-packages/torch/utils/data/sampler.py:165\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    161\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m     )\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    166\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_data_tensor = torch.tensor(train_data, dtype=torch.float32)\n",
    "test_data_tensor = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "# Convert labels to PyTorch tensors\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create DataLoader for training and testing\n",
    "train_dataset = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Model initialization\u001b[39;00m\n\u001b[32m      2\u001b[39m n_channels = \u001b[32m19\u001b[39m  \u001b[38;5;66;03m# Fixed for your data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m n_timesteps = \u001b[43mtrain_data\u001b[49m.shape[\u001b[32m2\u001b[39m]  \u001b[38;5;66;03m# Number of time points in each epoch\u001b[39;00m\n\u001b[32m      4\u001b[39m num_classes = \u001b[38;5;28mlen\u001b[39m(np.unique(train_labels))  \u001b[38;5;66;03m# Assuming labels are categorical, e.g., F, A, C\u001b[39;00m\n\u001b[32m      6\u001b[39m model = Amini_Adapted_CNN(n_channels=n_channels, n_timesteps=n_timesteps, num_classes=num_classes, dropout_rate=\u001b[32m0.5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Model initialization\n",
    "n_channels = 19  # Fixed for your data\n",
    "n_timesteps = train_data.shape[2]  # Number of time points in each epoch\n",
    "num_classes = len(np.unique(train_labels))  # Assuming labels are categorical, e.g., F, A, C\n",
    "\n",
    "model = Amini_Adapted_CNN(n_channels=n_channels, n_timesteps=n_timesteps, num_classes=num_classes, dropout_rate=0.5)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Model 2: EEGConvNeXt_1D...\n",
      "\n",
      "--- Training EEGConvNeXt_1D ---\n",
      "Epoch [1/100], Train Loss: 1.2604, Train Acc: 40.91%, Val Loss: 1.1359, Val Acc: 43.17%\n",
      "Epoch [2/100], Train Loss: 1.0956, Train Acc: 41.27%, Val Loss: 1.1550, Val Acc: 34.16%\n",
      "Epoch [3/100], Train Loss: 1.0957, Train Acc: 39.62%, Val Loss: 1.0769, Val Acc: 43.79%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m     criterion2 = nn.CrossEntropyLoss()\n\u001b[32m      9\u001b[39m     optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Add evaluation calls here if needed\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[32m     43\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m optimizer.step()\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Accumulate training statistics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Train Model 2: EEGConvNeXt_1D ---\n",
    "if len(train_loader) > 0:\n",
    "    try:\n",
    "        print(\"\\nInitializing Model 2: EEGConvNeXt_1D...\")\n",
    "        model2 = EEGConvNeXt_1D(n_channels=actual_n_channels,\n",
    "                                n_timesteps=actual_n_timesteps,\n",
    "                                num_classes=num_classes_actual).to(device)\n",
    "        criterion2 = nn.CrossEntropyLoss()\n",
    "        optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n",
    "        train_model(model2, train_loader, val_loader, criterion2, optimizer2, num_epochs, device)\n",
    "        # Add evaluation calls here if needed\n",
    "    except NameError:\n",
    "        print(\"Error: EEGConvNeXt_1D or PermuteLayerNorm class not defined.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Model 2 training: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Model 2 training: Train loader is empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing Model 3: EEGNet...\n",
      "\n",
      "--- Training EEGNet ---\n",
      "Epoch [1/100], Train Loss: 1.0656, Train Acc: 44.59%, Val Loss: 0.9740, Val Acc: 58.54%\n",
      "Epoch [2/100], Train Loss: 0.8774, Train Acc: 60.64%, Val Loss: 0.7874, Val Acc: 64.91%\n",
      "Epoch [3/100], Train Loss: 0.7665, Train Acc: 65.56%, Val Loss: 0.6953, Val Acc: 68.94%\n",
      "Epoch [4/100], Train Loss: 0.6856, Train Acc: 69.60%, Val Loss: 0.6231, Val Acc: 71.43%\n",
      "Epoch [5/100], Train Loss: 0.6241, Train Acc: 73.07%, Val Loss: 0.5845, Val Acc: 74.22%\n",
      "Epoch [6/100], Train Loss: 0.5807, Train Acc: 75.45%, Val Loss: 0.5478, Val Acc: 74.38%\n",
      "Epoch [7/100], Train Loss: 0.5234, Train Acc: 78.30%, Val Loss: 0.5306, Val Acc: 78.73%\n",
      "Epoch [8/100], Train Loss: 0.4840, Train Acc: 80.27%, Val Loss: 0.4608, Val Acc: 79.97%\n",
      "Epoch [9/100], Train Loss: 0.4642, Train Acc: 81.93%, Val Loss: 0.4112, Val Acc: 83.23%\n",
      "Epoch [10/100], Train Loss: 0.4013, Train Acc: 83.22%, Val Loss: 0.3705, Val Acc: 86.18%\n",
      "Epoch [11/100], Train Loss: 0.3884, Train Acc: 84.41%, Val Loss: 0.4310, Val Acc: 81.83%\n",
      "Epoch [12/100], Train Loss: 0.3186, Train Acc: 87.88%, Val Loss: 0.3519, Val Acc: 86.49%\n",
      "Epoch [13/100], Train Loss: 0.3377, Train Acc: 86.22%, Val Loss: 0.3051, Val Acc: 89.91%\n",
      "Epoch [14/100], Train Loss: 0.3091, Train Acc: 87.47%, Val Loss: 0.2903, Val Acc: 89.91%\n",
      "Epoch [15/100], Train Loss: 0.3055, Train Acc: 88.40%, Val Loss: 0.2821, Val Acc: 90.53%\n",
      "Epoch [16/100], Train Loss: 0.2952, Train Acc: 87.93%, Val Loss: 0.3265, Val Acc: 89.13%\n",
      "Epoch [17/100], Train Loss: 0.2843, Train Acc: 89.02%, Val Loss: 0.2786, Val Acc: 90.06%\n",
      "Epoch [18/100], Train Loss: 0.2728, Train Acc: 89.80%, Val Loss: 0.3172, Val Acc: 88.04%\n",
      "Epoch [19/100], Train Loss: 0.2655, Train Acc: 89.23%, Val Loss: 0.2608, Val Acc: 90.53%\n",
      "Epoch [20/100], Train Loss: 0.2528, Train Acc: 90.78%, Val Loss: 0.2829, Val Acc: 90.53%\n",
      "Epoch [21/100], Train Loss: 0.2699, Train Acc: 89.64%, Val Loss: 0.2741, Val Acc: 91.93%\n",
      "Epoch [22/100], Train Loss: 0.2239, Train Acc: 91.61%, Val Loss: 0.2368, Val Acc: 91.15%\n",
      "Epoch [23/100], Train Loss: 0.2227, Train Acc: 90.57%, Val Loss: 0.2379, Val Acc: 92.24%\n",
      "Epoch [24/100], Train Loss: 0.2435, Train Acc: 90.52%, Val Loss: 0.3120, Val Acc: 88.66%\n",
      "Epoch [25/100], Train Loss: 0.2336, Train Acc: 91.40%, Val Loss: 0.2236, Val Acc: 92.86%\n",
      "Epoch [26/100], Train Loss: 0.2146, Train Acc: 91.14%, Val Loss: 0.2171, Val Acc: 92.86%\n",
      "Epoch [27/100], Train Loss: 0.2299, Train Acc: 91.20%, Val Loss: 0.2083, Val Acc: 93.63%\n",
      "Epoch [28/100], Train Loss: 0.1886, Train Acc: 92.49%, Val Loss: 0.2271, Val Acc: 92.08%\n",
      "Epoch [29/100], Train Loss: 0.2185, Train Acc: 91.97%, Val Loss: 0.2086, Val Acc: 93.63%\n",
      "Epoch [30/100], Train Loss: 0.1963, Train Acc: 92.49%, Val Loss: 0.2174, Val Acc: 92.86%\n",
      "Epoch [31/100], Train Loss: 0.1953, Train Acc: 92.91%, Val Loss: 0.1958, Val Acc: 93.48%\n",
      "Epoch [32/100], Train Loss: 0.2025, Train Acc: 92.49%, Val Loss: 0.2257, Val Acc: 91.30%\n",
      "Epoch [33/100], Train Loss: 0.1997, Train Acc: 92.54%, Val Loss: 0.2003, Val Acc: 93.32%\n",
      "Epoch [34/100], Train Loss: 0.1625, Train Acc: 93.73%, Val Loss: 0.3412, Val Acc: 86.65%\n",
      "Epoch [35/100], Train Loss: 0.1963, Train Acc: 92.59%, Val Loss: 0.1907, Val Acc: 93.17%\n",
      "Epoch [36/100], Train Loss: 0.1668, Train Acc: 93.84%, Val Loss: 0.1972, Val Acc: 93.79%\n",
      "Epoch [37/100], Train Loss: 0.1562, Train Acc: 93.94%, Val Loss: 0.1846, Val Acc: 93.17%\n",
      "Epoch [38/100], Train Loss: 0.1703, Train Acc: 93.73%, Val Loss: 0.2239, Val Acc: 93.17%\n",
      "Epoch [39/100], Train Loss: 0.1364, Train Acc: 95.08%, Val Loss: 0.2169, Val Acc: 93.01%\n",
      "Epoch [40/100], Train Loss: 0.1857, Train Acc: 93.37%, Val Loss: 0.3130, Val Acc: 87.11%\n",
      "Epoch [41/100], Train Loss: 0.1685, Train Acc: 93.73%, Val Loss: 0.1857, Val Acc: 93.94%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m         criterion3 = nn.CrossEntropyLoss()\n\u001b[32m     20\u001b[39m         optimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m         \u001b[38;5;66;03m# Add evaluation calls here if needed\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[32m     43\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m optimizer.step()\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Accumulate training statistics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Train Model 3: EEGNet ---\n",
    "if len(train_loader) > 0:\n",
    "    try:\n",
    "        print(\"\\nInitializing Model 3: EEGNet...\")\n",
    "        if actual_n_timesteps // 32 <= 0:\n",
    "                print(f\"Warning: n_timesteps ({actual_n_timesteps}) might be too small for EEGNet pooling. Skipping EEGNet.\")\n",
    "        else:\n",
    "            # EEGNet parameters\n",
    "            F1=8; D=2; F2=F1*D\n",
    "            kernel_length = min(64, actual_n_timesteps // 4) # Adjust kernel based on actual timesteps, e.g. fs/4\n",
    "            dropout_rate=0.25\n",
    "\n",
    "            model3 = EEGNet(n_channels=actual_n_channels,\n",
    "                            n_timesteps=actual_n_timesteps,\n",
    "                            num_classes=num_classes_actual,\n",
    "                            F1=F1, D=D, F2=F2,\n",
    "                            kernel_length=kernel_length,\n",
    "                            dropout_rate=dropout_rate).to(device)\n",
    "            criterion3 = nn.CrossEntropyLoss()\n",
    "            optimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)\n",
    "            train_model(model3, train_loader, val_loader, criterion3, optimizer3, num_epochs, device)\n",
    "            # Add evaluation calls here if needed\n",
    "    except NameError:\n",
    "        print(\"Error: EEGNet class not defined.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Model 3 training: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Model 3 training: Train loader is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model's accuracy on the provided data loader.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained PyTorch model to evaluate.\n",
    "        data_loader (DataLoader): DataLoader for the dataset to evaluate on (e.g., test_loader or val_loader).\n",
    "        device (torch.device): The device to run evaluation on (CPU or CUDA).\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the dataset (in percentage).\n",
    "               Returns 0.0 if the data_loader is empty or None.\n",
    "    \"\"\"\n",
    "    if data_loader is None or len(data_loader) == 0:\n",
    "        print(\"Warning: Evaluation data loader is empty or None. Returning 0.0 accuracy.\")\n",
    "        return 0.0\n",
    "\n",
    "    model.to(device) # Ensure model is on the correct device\n",
    "    model.eval()     # Set the model to evaluation mode (disables dropout, uses running means/vars for BatchNorm)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculations - crucial for evaluation efficiency and correctness\n",
    "        for inputs, labels in data_loader:\n",
    "            # Move data to the designated device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get predictions from the maximum value\n",
    "            _, predicted = torch.max(outputs.data, 1) # Get the index of the max log-probability/logit\n",
    "\n",
    "            # Update total samples and correct predictions count\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "    # Calculate final accuracy\n",
    "    accuracy = 100 * correct_predictions / total_samples\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.3416149068323"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model3, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
